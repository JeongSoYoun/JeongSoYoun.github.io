<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Yoon's Dev ]]></title><description><![CDATA[Blog posted about ...]]></description><link>https://jeongsoyoun.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 26 Apr 2022 14:29:18 GMT</lastBuildDate><item><title><![CDATA[CosmWasm]]></title><link>https://jeongsoyoun.github.io/Portfolio/cosmwasm/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/cosmwasm/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Polkadot Dev Camp]]></title><description><![CDATA[Polkadot Official Dev Camp]]></description><link>https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Polkadot Official Dev Camp&lt;/p&gt;</content:encoded></item><item><title><![CDATA[GPT Model]]></title><description><![CDATA[GPT Model for NLP]]></description><link>https://jeongsoyoun.github.io/ML/AI/gpt_2022_04-25/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/gpt_2022_04-25/</guid><pubDate>Mon, 25 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;GPT Model for NLP&lt;/p&gt;</content:encoded></item><item><title><![CDATA[about]]></title><description><![CDATA[Thank You !]]></description><link>https://jeongsoyoun.github.io/resume-en/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/resume-en/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;thank-you-&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#thank-you-&quot; aria-label=&quot;thank you  permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thank You !&lt;/h1&gt;
&lt;div&gt;
    Contact
&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Uniswap Core Review]]></title><link>https://jeongsoyoun.github.io/Blockchain/uniswap/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/uniswap/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Bitcoin White Paper]]></title><description><![CDATA[Bitcoin White Paper by Satoshi Nakamoto]]></description><link>https://jeongsoyoun.github.io/Blockchain/btc_white_paper/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/btc_white_paper/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Bitcoin White Paper by Satoshi Nakamoto&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Transformer Model]]></title><description><![CDATA[Transformer Model: Attention is all you Need Transformer model was published by Google AI on 2017, and became fundamental of Naturalâ€¦]]></description><link>https://jeongsoyoun.github.io/ML/AI/transformer_2022_04_18/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/transformer_2022_04_18/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;br/&gt;&lt;img src=&quot;/Users/yoon/dev/JeongSoYoun.github.io/my-blog/content/assets/blog/google.png&quot; width=&quot;600&quot; height=&quot;300&quot;&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;transformer-model-attention-is-all-you-need&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#transformer-model-attention-is-all-you-need&quot; aria-label=&quot;transformer model attention is all you need permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Transformer Model: Attention is all you Need&lt;/em&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;Transformer model was published by &lt;strong&gt;Google AI&lt;/strong&gt; on 2017, and became fundamental of Natural Language Process Model for the other models such as &lt;em&gt;BERT&lt;/em&gt; or &lt;em&gt;GPT&lt;/em&gt;. What makes Transformer model attractive is, this model never uses &lt;strong&gt;RNN&lt;/strong&gt; architecture. It only uses &lt;strong&gt;Attention&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;Our language has some meaningful connection between words, which we called &lt;em&gt;context&lt;/em&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[SubStake]]></title><link>https://jeongsoyoun.github.io/Portfolio/substake/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/substake/</guid><pubDate>Sun, 14 Jun 2020 16:21:13 GMT</pubDate><content:encoded></content:encoded></item></channel></rss>