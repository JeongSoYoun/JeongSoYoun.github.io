---
title: 'ðŸ¤– Transformer Model'
date: 2022-04-18 00:00:00
category: 'ML/AI'
tags: ['Deep Learning', 'NLP', 'PyTorch']
summary: 'Transformer Model Paper Review'
image: 'transformer'
draft: false
---

<br /><img src="./images/google.png" width="600" height="300">

## Introduction

<br />

> ### _Transformer Model: Attention is all you Need_

<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer model was published by **Google AI** on 2017, and became fundamental of Natural Language Process Model for the other models such as _BERT_ or _GPT_. What makes Transformer model attractive is, this model never uses **RNN**, one of the greatest sequence model, architecture. It only uses **Attention**.

<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Our language has some meaningful connection between words, which we called _context_. For human, we can easily guess what _it_ stands for, but for machine learning points of view, it is hard to train what _it_ means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where **Attention** shines, and **Transformer** model only uses this technique.

## Architecture

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;About Transformer model Arcitecture

## Conclusion

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer is beautiful!
