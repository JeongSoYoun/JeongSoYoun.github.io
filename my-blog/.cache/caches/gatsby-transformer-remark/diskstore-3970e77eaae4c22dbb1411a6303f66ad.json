{"expireTime":9007200905724147000,"key":"transformer-remark-markdown-ast-f5fdbbcb742e89e51ca28bb314cd1e4b-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"paragraph","children":[{"type":"image","title":null,"url":"/assets/blog/tux.png","alt":"Tux, the Linux mascot","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":47,"offset":47},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":47,"offset":47},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#transformer-model-attention-is-all-you-need","title":null,"children":[],"data":{"hProperties":{"aria-label":"transformer model attention is all you need permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"emphasis","children":[{"type":"text","value":"Transformer Model: Attention is all you Need","position":{"start":{"line":4,"column":8,"offset":56},"end":{"line":4,"column":52,"offset":100},"indent":[]}}],"position":{"start":{"line":4,"column":7,"offset":55},"end":{"line":4,"column":53,"offset":101},"indent":[]}}],"position":{"start":{"line":4,"column":3,"offset":51},"end":{"line":4,"column":53,"offset":101},"indent":[]},"data":{"id":"transformer-model-attention-is-all-you-need","htmlAttributes":{"id":"transformer-model-attention-is-all-you-need"},"hProperties":{"id":"transformer-model-attention-is-all-you-need","style":"position:relative;"}}}],"position":{"start":{"line":4,"column":1,"offset":49},"end":{"line":4,"column":53,"offset":101},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":6,"column":1,"offset":103},"end":{"line":6,"column":7,"offset":109},"indent":[]}},{"type":"text","value":"Transformer model was published by ","position":{"start":{"line":6,"column":7,"offset":109},"end":{"line":6,"column":42,"offset":144},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Google AI","position":{"start":{"line":6,"column":44,"offset":146},"end":{"line":6,"column":53,"offset":155},"indent":[]}}],"position":{"start":{"line":6,"column":42,"offset":144},"end":{"line":6,"column":55,"offset":157},"indent":[]}},{"type":"text","value":" on 2017, and became fundamental of Natural Language Process Model for the other models such as ","position":{"start":{"line":6,"column":55,"offset":157},"end":{"line":6,"column":151,"offset":253},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"BERT","position":{"start":{"line":6,"column":152,"offset":254},"end":{"line":6,"column":156,"offset":258},"indent":[]}}],"position":{"start":{"line":6,"column":151,"offset":253},"end":{"line":6,"column":157,"offset":259},"indent":[]}},{"type":"text","value":" or ","position":{"start":{"line":6,"column":157,"offset":259},"end":{"line":6,"column":161,"offset":263},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":6,"column":162,"offset":264},"end":{"line":6,"column":165,"offset":267},"indent":[]}}],"position":{"start":{"line":6,"column":161,"offset":263},"end":{"line":6,"column":166,"offset":268},"indent":[]}},{"type":"text","value":". What makes Transformer model attractive is, this model never uses ","position":{"start":{"line":6,"column":166,"offset":268},"end":{"line":6,"column":234,"offset":336},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"RNN","position":{"start":{"line":6,"column":236,"offset":338},"end":{"line":6,"column":239,"offset":341},"indent":[]}}],"position":{"start":{"line":6,"column":234,"offset":336},"end":{"line":6,"column":241,"offset":343},"indent":[]}},{"type":"text","value":" architecture. It only uses ","position":{"start":{"line":6,"column":241,"offset":343},"end":{"line":6,"column":269,"offset":371},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Attention","position":{"start":{"line":6,"column":271,"offset":373},"end":{"line":6,"column":280,"offset":382},"indent":[]}}],"position":{"start":{"line":6,"column":269,"offset":371},"end":{"line":6,"column":282,"offset":384},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":6,"column":282,"offset":384},"end":{"line":6,"column":283,"offset":385},"indent":[]}}],"position":{"start":{"line":6,"column":1,"offset":103},"end":{"line":6,"column":283,"offset":385},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br/>","position":{"start":{"line":8,"column":1,"offset":387},"end":{"line":8,"column":6,"offset":392},"indent":[]}},{"type":"text","value":"Our language has some meaningful connection between words, which we called ","position":{"start":{"line":8,"column":6,"offset":392},"end":{"line":8,"column":81,"offset":467},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"context","position":{"start":{"line":8,"column":82,"offset":468},"end":{"line":8,"column":89,"offset":475},"indent":[]}}],"position":{"start":{"line":8,"column":81,"offset":467},"end":{"line":8,"column":90,"offset":476},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":8,"column":90,"offset":476},"end":{"line":8,"column":91,"offset":477},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":387},"end":{"line":8,"column":91,"offset":477},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":9,"column":1,"offset":478}}}}