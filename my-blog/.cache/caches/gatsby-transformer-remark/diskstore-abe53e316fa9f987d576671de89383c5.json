{"expireTime":9007200906165276000,"key":"transformer-remark-markdown-ast-02f4a8f0ee22630f1aa7f966a562aa88-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7},"indent":[]}},{"type":"html","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAADEElEQVQozx2Si1PSBwCAf3/XdrdVt0e3re20dl5b2qlnE0UReU4FlfmIYfIwYCEIqIFgonCiTmjNlrHZiktNapooGD43a95NduZ94/YPfHfffZ/wxd0tSqaylIUyVPk2ELvXkdnXaDI+Q9f5GH3zA8yNcRzVUYZLQ4Qu+pn5xMsvH7h4fK6f1FkHmTM/cPC+jeMzNoSqyTQ1E5tI7mwh9+f4dnAbjTOHri9Nt36ZntYEFkUMhzjMcGWQ0FdDzF50Mn/BTvJ8Hy8+NJI9+z1/vtfF8TvtCJLRl0iDmf9hCm8OlXOH1v4duqwZDD0pTLoFbOo4LkkYnyhIuGKQ+JV+EiU2nhabWP1Mz6vz3/HXuWby78oQaj0FzYEMMncWR3QPe2iHDvsGHYbn9HQkuamZx6WcxScNExIHmK4eYq7KyUKljcUrvaxd7uZVUQuHn6vIf1SHcNWS4xtLlsEf9xiL7zIzt8vtwDoO+zLxiTVG9A8Za71LRBEhph7nvirIgmqIRc0Q6wY/GZ2T184Qh6VK8pdqEUo69hAZtgjF93EFNvGNppmZShOLrBF2PGH53jpLkymeR1dIx1Jszi6RmfiNrYl5Dh8+JWfychSM8Lqynn8rqhHKNPtc02Rx+beJzmwzXSg+OriC3/6Ee8FFEuNLpOIpkiMJ1qK/84cnTto9zV4wxoF7jDe3xzi+M84/9WJOROWFyvIDahqzqNSrWE2rOIzLdEp/pqdhBqt0nOGWwiqtI4TqbPzUaOORzMILhZEtmZ4DaRtHyibyigZO5Nd4K72MIBLtI6nZQFb9DGXVI7TV9+kSz9JbF8EqCTAg8eCX3mJSZWNO3UeyyczL5l52Wwy80XZyrG3mpK2Bt23lnLYVI9SX7iItW0V5NUlz+Txt5TE6K8IYqgLcFHlx1/cTlFmZUlt4oDWxqDOS7rzBXreBv/Vd5A1aTnqlnJoLQPOnCLKiHPKiFZTFv9J0KY7myzDtJT6ufz1AX8UtPLVWxgqaMY2ZRLeJZZORTfsN9p09HA3oybvbOfHIOfUWgJ6P+Q9k0XsmDcsekgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"openai\" title=\"openai\" src=\"/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png\" srcset=\"/static/7c1f9fb711eecac5f959f784796c2791/5a46d/openai.png 300w,\n/static/7c1f9fb711eecac5f959f784796c2791/0a47e/openai.png 600w,\n/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png 1200w,\n/static/7c1f9fb711eecac5f959f784796c2791/229ad/openai.png 1356w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":4,"column":7,"offset":71},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#introduction","title":null,"children":[],"data":{"hProperties":{"aria-label":"introduction permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Introduction","position":{"start":{"line":6,"column":7,"offset":79},"end":{"line":6,"column":19,"offset":91},"indent":[]}}],"position":{"start":{"line":6,"column":3,"offset":75},"end":{"line":6,"column":19,"offset":91},"indent":[]},"data":{"id":"introduction","htmlAttributes":{"id":"introduction"},"hProperties":{"id":"introduction","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"From Transformer model with ","position":{"start":{"line":8,"column":3,"offset":96},"end":{"line":8,"column":31,"offset":124},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"attention","position":{"start":{"line":8,"column":32,"offset":125},"end":{"line":8,"column":41,"offset":134},"indent":[]}}],"position":{"start":{"line":8,"column":31,"offset":124},"end":{"line":8,"column":42,"offset":135},"indent":[]}},{"type":"text","value":" , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about ","position":{"start":{"line":8,"column":42,"offset":135},"end":{"line":8,"column":319,"offset":412},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":8,"column":322,"offset":415},"end":{"line":8,"column":325,"offset":418},"indent":[]}}],"position":{"start":{"line":8,"column":321,"offset":414},"end":{"line":8,"column":326,"offset":419},"indent":[]}}],"position":{"start":{"line":8,"column":319,"offset":412},"end":{"line":8,"column":328,"offset":421},"indent":[]}},{"type":"text","value":" model.","position":{"start":{"line":8,"column":328,"offset":421},"end":{"line":8,"column":335,"offset":428},"indent":[]}}],"position":{"start":{"line":8,"column":3,"offset":96},"end":{"line":8,"column":335,"offset":428},"indent":[]}}],"position":{"start":{"line":6,"column":1,"offset":73},"end":{"line":8,"column":335,"offset":428},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":10,"column":1,"offset":430},"end":{"line":10,"column":7,"offset":436},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":7,"offset":436},"end":{"line":10,"column":8,"offset":437},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":8,"offset":437},"end":{"line":10,"column":14,"offset":443},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":14,"offset":443},"end":{"line":10,"column":20,"offset":449},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":20,"offset":449},"end":{"line":10,"column":26,"offset":455},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":26,"offset":455},"end":{"line":10,"column":32,"offset":461},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":10,"column":32,"offset":461},"end":{"line":10,"column":38,"offset":467},"indent":[]}},{"type":"text","value":" GPT, ","position":{"start":{"line":10,"column":38,"offset":467},"end":{"line":10,"column":44,"offset":473},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Generative Pre Training","position":{"start":{"line":10,"column":45,"offset":474},"end":{"line":10,"column":68,"offset":497},"indent":[]}}],"position":{"start":{"line":10,"column":44,"offset":473},"end":{"line":10,"column":69,"offset":498},"indent":[]}},{"type":"text","value":", is a language model developed by ","position":{"start":{"line":10,"column":69,"offset":498},"end":{"line":10,"column":104,"offset":533},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Open AI","position":{"start":{"line":10,"column":106,"offset":535},"end":{"line":10,"column":113,"offset":542},"indent":[]}}],"position":{"start":{"line":10,"column":104,"offset":533},"end":{"line":10,"column":115,"offset":544},"indent":[]}},{"type":"text","value":" with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).","position":{"start":{"line":10,"column":115,"offset":544},"end":{"line":10,"column":350,"offset":779},"indent":[]}}],"position":{"start":{"line":10,"column":1,"offset":430},"end":{"line":10,"column":350,"offset":779},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":12,"column":1,"offset":781},"end":{"line":12,"column":7,"offset":787},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#trainig-process","title":null,"children":[],"data":{"hProperties":{"aria-label":"trainig process permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Trainig Process","position":{"start":{"line":14,"column":7,"offset":795},"end":{"line":14,"column":22,"offset":810},"indent":[]}}],"position":{"start":{"line":14,"column":3,"offset":791},"end":{"line":14,"column":22,"offset":810},"indent":[]},"data":{"id":"trainig-process","htmlAttributes":{"id":"trainig-process"},"hProperties":{"id":"trainig-process","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"From pre-training to fine-tuning","position":{"start":{"line":16,"column":3,"offset":815},"end":{"line":16,"column":35,"offset":847},"indent":[]}}],"position":{"start":{"line":16,"column":3,"offset":815},"end":{"line":16,"column":35,"offset":847},"indent":[]}}],"position":{"start":{"line":14,"column":1,"offset":789},"end":{"line":16,"column":35,"offset":847},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":18,"column":1,"offset":849},"end":{"line":18,"column":7,"offset":855},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":7,"offset":855},"end":{"line":18,"column":8,"offset":856},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":8,"offset":856},"end":{"line":18,"column":14,"offset":862},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":14,"offset":862},"end":{"line":18,"column":20,"offset":868},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":20,"offset":868},"end":{"line":18,"column":26,"offset":874},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":26,"offset":874},"end":{"line":18,"column":32,"offset":880},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":18,"column":32,"offset":880},"end":{"line":18,"column":38,"offset":886},"indent":[]}},{"type":"text","value":" First, pre-training(unsupervised learning with set of large-corpus of text data) is done to learn the parameters of language model. Since, the goal of unsupervised learning is to initialize the training, the objectives of learning remain same with supervised learning. After pre-training, supervised learning with same parameters that have pre-trained is processed with fine-tuning of some change of input tokens and the weight of the output layer.","position":{"start":{"line":18,"column":38,"offset":886},"end":{"line":18,"column":487,"offset":1335},"indent":[]}}],"position":{"start":{"line":18,"column":1,"offset":849},"end":{"line":18,"column":487,"offset":1335},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":20,"column":1,"offset":1337},"end":{"line":20,"column":7,"offset":1343},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#architecture","title":null,"children":[],"data":{"hProperties":{"aria-label":"architecture permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Architecture","position":{"start":{"line":22,"column":7,"offset":1351},"end":{"line":22,"column":19,"offset":1363},"indent":[]}}],"position":{"start":{"line":22,"column":3,"offset":1347},"end":{"line":22,"column":19,"offset":1363},"indent":[]},"data":{"id":"architecture","htmlAttributes":{"id":"architecture"},"hProperties":{"id":"architecture","style":"position:relative;"}}}],"position":{"start":{"line":22,"column":1,"offset":1345},"end":{"line":22,"column":19,"offset":1363},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1365},"end":{"line":24,"column":7,"offset":1371},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":7,"offset":1371},"end":{"line":24,"column":8,"offset":1372},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":8,"offset":1372},"end":{"line":24,"column":14,"offset":1378},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":14,"offset":1378},"end":{"line":24,"column":20,"offset":1384},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":20,"offset":1384},"end":{"line":24,"column":26,"offset":1390},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":26,"offset":1390},"end":{"line":24,"column":32,"offset":1396},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":24,"column":32,"offset":1396},"end":{"line":24,"column":38,"offset":1402},"indent":[]}},{"type":"text","value":" From pre-training to fine-tuning, architecture of GPT is ","position":{"start":{"line":24,"column":38,"offset":1402},"end":{"line":24,"column":96,"offset":1460},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"Transformer Model","position":{"start":{"line":24,"column":99,"offset":1463},"end":{"line":24,"column":116,"offset":1480},"indent":[]}}],"position":{"start":{"line":24,"column":98,"offset":1462},"end":{"line":24,"column":117,"offset":1481},"indent":[]}}],"position":{"start":{"line":24,"column":96,"offset":1460},"end":{"line":24,"column":119,"offset":1483},"indent":[]}},{"type":"text","value":". Interesting point is ","position":{"start":{"line":24,"column":119,"offset":1483},"end":{"line":24,"column":142,"offset":1506},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":24,"column":143,"offset":1507},"end":{"line":24,"column":146,"offset":1510},"indent":[]}}],"position":{"start":{"line":24,"column":142,"offset":1506},"end":{"line":24,"column":147,"offset":1511},"indent":[]}},{"type":"text","value":" only uses ","position":{"start":{"line":24,"column":147,"offset":1511},"end":{"line":24,"column":158,"offset":1522},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Decoder","position":{"start":{"line":24,"column":160,"offset":1524},"end":{"line":24,"column":167,"offset":1531},"indent":[]}}],"position":{"start":{"line":24,"column":158,"offset":1522},"end":{"line":24,"column":169,"offset":1533},"indent":[]}},{"type":"text","value":" layer which has ","position":{"start":{"line":24,"column":169,"offset":1533},"end":{"line":24,"column":186,"offset":1550},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"masked self attention layer","position":{"start":{"line":24,"column":187,"offset":1551},"end":{"line":24,"column":214,"offset":1578},"indent":[]}}],"position":{"start":{"line":24,"column":186,"offset":1550},"end":{"line":24,"column":215,"offset":1579},"indent":[]}},{"type":"text","value":". Objective of pre-training is given a sequence of words with unlabeled dataset U(u","position":{"start":{"line":24,"column":215,"offset":1579},"end":{"line":24,"column":298,"offset":1662},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"1 … u","position":{"start":{"line":24,"column":299,"offset":1663},"end":{"line":24,"column":306,"offset":1670},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"l) to maximize the likelihood of the probability to predict the word of position \\","position":{"start":{"line":24,"column":307,"offset":1671},"end":{"line":24,"column":389,"offset":1753},"indent":[]}}],"position":{"start":{"line":24,"column":306,"offset":1670},"end":{"line":24,"column":390,"offset":1754},"indent":[]}},{"type":"text","value":"l","position":{"start":{"line":24,"column":390,"offset":1754},"end":{"line":24,"column":391,"offset":1755},"indent":[]}}],"position":{"start":{"line":24,"column":298,"offset":1662},"end":{"line":24,"column":392,"offset":1756},"indent":[]}},{"type":"text","value":" given ","position":{"start":{"line":24,"column":392,"offset":1756},"end":{"line":24,"column":399,"offset":1763},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"1 … l-1","position":{"start":{"line":24,"column":400,"offset":1764},"end":{"line":24,"column":409,"offset":1773},"indent":[]}}],"position":{"start":{"line":24,"column":399,"offset":1763},"end":{"line":24,"column":410,"offset":1774},"indent":[]}},{"type":"text","value":" words","position":{"start":{"line":24,"column":410,"offset":1774},"end":{"line":24,"column":416,"offset":1780},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":1365},"end":{"line":24,"column":416,"offset":1780},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":26,"column":1,"offset":1782},"end":{"line":26,"column":7,"offset":1788},"indent":[]}},{"type":"text","value":" After pre-training, sequence of input data (x","position":{"start":{"line":26,"column":7,"offset":1788},"end":{"line":26,"column":53,"offset":1834},"indent":[]}},{"type":"text","value":"*","position":{"start":{"line":26,"column":53,"offset":1834},"end":{"line":26,"column":55,"offset":1836},"indent":[]}},{"type":"text","value":"1 to x","position":{"start":{"line":26,"column":55,"offset":1836},"end":{"line":26,"column":61,"offset":1842},"indent":[]}},{"type":"text","value":"*","position":{"start":{"line":26,"column":61,"offset":1842},"end":{"line":26,"column":63,"offset":1844},"indent":[]}},{"type":"text","value":"m), which would be the first fine-tuning process with ","position":{"start":{"line":26,"column":63,"offset":1844},"end":{"line":26,"column":117,"offset":1898},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"dlim token","position":{"start":{"line":26,"column":118,"offset":1899},"end":{"line":26,"column":128,"offset":1909},"indent":[]}}],"position":{"start":{"line":26,"column":117,"offset":1898},"end":{"line":26,"column":129,"offset":1910},"indent":[]}},{"type":"text","value":", with label y is given and pass it through pre-trained Transformer model. Then, the output will go through the output linear layer and this would be the second fine-tuning process.","position":{"start":{"line":26,"column":129,"offset":1910},"end":{"line":26,"column":310,"offset":2091},"indent":[]}}],"position":{"start":{"line":26,"column":1,"offset":1782},"end":{"line":26,"column":310,"offset":2091},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":27,"column":1,"offset":2092}}}}