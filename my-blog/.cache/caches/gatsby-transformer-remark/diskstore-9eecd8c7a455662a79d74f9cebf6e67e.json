{"expireTime":9007200906039504000,"key":"transformer-remark-markdown-html-4e9c1a26a3b43221a8eb301321f64c5a-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":"<p>From Transformer model with <em>attention</em> , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about <strong><em>GPT</em></strong> model.</p>\n<p>GPT, Generative Pre Training, is a language model with combination of generative pre-training and discriminative fine-tuning process.</p>"}