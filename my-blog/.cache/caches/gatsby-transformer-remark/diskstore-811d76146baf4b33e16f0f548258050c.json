{"expireTime":9007200906038602000,"key":"transformer-remark-markdown-html-ast-e9738528e1609b5507701199e46c15cd-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7}}},{"type":"raw","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz6WRX0/CMBTF+f5fQh/gnSiDESHCg9FgjAlGJeFPhKVra2xj1rK1dm3tcOjCQAKcNGlzkt/tvedW7Amq5LcxxmbnKDgvkdFmXW1Dv2YJ/pKCz5dqmTexWxuwkdrGIaWgWX+eMG6HT8P5IkgSEceJE2NsNnvjnGutx5MpRFgI8Qcn2kaLDxq2a4/TAPDB/aB73b/0Wn670/KvGp5/dl6tX3ghRM65ub1j7odVCxmcWqto9In6DyBwLsIYAIjxO0QIQtTp9l5eR4SQKGJSSkJomqalwLSw29JWSm0duLiqFb6O1I33E6x7FJMvB1ZY1L54/93zgfoGgyp+WM2mC3AAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"google\" title=\"google\" src=\"/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png\" srcset=\"/static/fbc6dd92a8903039c6d1223bd49971b7/5a46d/google.png 300w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/0a47e/google.png 600w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png 1200w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63}}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"introduction","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#introduction","aria-label":"introduction permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Introduction","position":{"start":{"line":4,"column":4,"offset":68},"end":{"line":4,"column":16,"offset":80}}}],"position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":4,"column":16,"offset":80}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":6,"column":1,"offset":82},"end":{"line":6,"column":7,"offset":88}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"transformer-model-attention-is-all-you-need","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#transformer-model-attention-is-all-you-need","aria-label":"transformer model attention is all you need permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Transformer Model: Attention is all you Need","position":{"start":{"line":8,"column":6,"offset":95},"end":{"line":8,"column":50,"offset":139}}}],"position":{"start":{"line":8,"column":5,"offset":94},"end":{"line":8,"column":51,"offset":140}}}],"position":{"start":{"line":8,"column":1,"offset":90},"end":{"line":8,"column":51,"offset":140}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":1,"offset":142},"end":{"line":10,"column":7,"offset":148}}},{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":7,"offset":148},"end":{"line":10,"column":13,"offset":154}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":13,"offset":154},"end":{"line":10,"column":19,"offset":160}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":19,"offset":160},"end":{"line":10,"column":25,"offset":166}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":25,"offset":166},"end":{"line":10,"column":31,"offset":172}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":31,"offset":172},"end":{"line":10,"column":37,"offset":178}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":37,"offset":178},"end":{"line":10,"column":43,"offset":184}}},{"type":"text","value":"Transformer model was published by ","position":{"start":{"line":10,"column":43,"offset":184},"end":{"line":10,"column":78,"offset":219}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Google AI","position":{"start":{"line":10,"column":80,"offset":221},"end":{"line":10,"column":89,"offset":230}}}],"position":{"start":{"line":10,"column":78,"offset":219},"end":{"line":10,"column":91,"offset":232}}},{"type":"text","value":" on 2017, and became fundamental of Natural Language Process Model for the other models such as ","position":{"start":{"line":10,"column":91,"offset":232},"end":{"line":10,"column":187,"offset":328}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"BERT","position":{"start":{"line":10,"column":188,"offset":329},"end":{"line":10,"column":192,"offset":333}}}],"position":{"start":{"line":10,"column":187,"offset":328},"end":{"line":10,"column":193,"offset":334}}},{"type":"text","value":" or ","position":{"start":{"line":10,"column":193,"offset":334},"end":{"line":10,"column":197,"offset":338}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"GPT","position":{"start":{"line":10,"column":198,"offset":339},"end":{"line":10,"column":201,"offset":342}}}],"position":{"start":{"line":10,"column":197,"offset":338},"end":{"line":10,"column":202,"offset":343}}},{"type":"text","value":". What makes Transformer model attractive is, this model never uses ","position":{"start":{"line":10,"column":202,"offset":343},"end":{"line":10,"column":270,"offset":411}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"RNN","position":{"start":{"line":10,"column":272,"offset":413},"end":{"line":10,"column":275,"offset":416}}}],"position":{"start":{"line":10,"column":270,"offset":411},"end":{"line":10,"column":277,"offset":418}}},{"type":"text","value":", one of the greatest sequence model, architecture. It only uses ","position":{"start":{"line":10,"column":277,"offset":418},"end":{"line":10,"column":342,"offset":483}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Attention","position":{"start":{"line":10,"column":344,"offset":485},"end":{"line":10,"column":353,"offset":494}}}],"position":{"start":{"line":10,"column":342,"offset":483},"end":{"line":10,"column":355,"offset":496}}},{"type":"text","value":".","position":{"start":{"line":10,"column":355,"offset":496},"end":{"line":10,"column":356,"offset":497}}}],"position":{"start":{"line":10,"column":1,"offset":142},"end":{"line":10,"column":356,"offset":497}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":12,"column":1,"offset":499},"end":{"line":12,"column":7,"offset":505}}},{"type":"text","value":" ","position":{"start":{"line":12,"column":7,"offset":505},"end":{"line":12,"column":13,"offset":511}}},{"type":"text","value":" ","position":{"start":{"line":12,"column":13,"offset":511},"end":{"line":12,"column":19,"offset":517}}},{"type":"text","value":" ","position":{"start":{"line":12,"column":19,"offset":517},"end":{"line":12,"column":25,"offset":523}}},{"type":"text","value":" ","position":{"start":{"line":12,"column":25,"offset":523},"end":{"line":12,"column":31,"offset":529}}},{"type":"text","value":" ","position":{"start":{"line":12,"column":31,"offset":529},"end":{"line":12,"column":37,"offset":535}}},{"type":"text","value":"Our language has meaningful relationship in sentence, which we called ","position":{"start":{"line":12,"column":37,"offset":535},"end":{"line":12,"column":107,"offset":605}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"context","position":{"start":{"line":12,"column":108,"offset":606},"end":{"line":12,"column":115,"offset":613}}}],"position":{"start":{"line":12,"column":107,"offset":605},"end":{"line":12,"column":116,"offset":614}}},{"type":"text","value":". For human, we can easily guess what ","position":{"start":{"line":12,"column":116,"offset":614},"end":{"line":12,"column":154,"offset":652}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":155,"offset":653},"end":{"line":12,"column":157,"offset":655}}}],"position":{"start":{"line":12,"column":154,"offset":652},"end":{"line":12,"column":158,"offset":656}}},{"type":"text","value":" stands for, but for machine learning points of view, it is hard to train what ","position":{"start":{"line":12,"column":158,"offset":656},"end":{"line":12,"column":237,"offset":735}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":238,"offset":736},"end":{"line":12,"column":240,"offset":738}}}],"position":{"start":{"line":12,"column":237,"offset":735},"end":{"line":12,"column":241,"offset":739}}},{"type":"text","value":" means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where ","position":{"start":{"line":12,"column":241,"offset":739},"end":{"line":12,"column":465,"offset":963}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Attention","position":{"start":{"line":12,"column":467,"offset":965},"end":{"line":12,"column":476,"offset":974}}}],"position":{"start":{"line":12,"column":465,"offset":963},"end":{"line":12,"column":478,"offset":976}}},{"type":"text","value":" shines, and ","position":{"start":{"line":12,"column":478,"offset":976},"end":{"line":12,"column":491,"offset":989}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Transformer","position":{"start":{"line":12,"column":493,"offset":991},"end":{"line":12,"column":504,"offset":1002}}}],"position":{"start":{"line":12,"column":491,"offset":989},"end":{"line":12,"column":506,"offset":1004}}},{"type":"text","value":" model only uses this technique.","position":{"start":{"line":12,"column":506,"offset":1004},"end":{"line":12,"column":538,"offset":1036}}}],"position":{"start":{"line":12,"column":1,"offset":499},"end":{"line":12,"column":538,"offset":1036}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"architecture","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#architecture","aria-label":"architecture permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Architecture","position":{"start":{"line":14,"column":4,"offset":1041},"end":{"line":14,"column":16,"offset":1053}}}],"position":{"start":{"line":14,"column":1,"offset":1038},"end":{"line":14,"column":16,"offset":1053}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":16,"column":1,"offset":1055},"end":{"line":16,"column":7,"offset":1061}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"input---encoder---deocder---output","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#input---encoder---deocder---output","aria-label":"input   encoder   deocder   output permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Input - Encoder - Deocder - Output","position":{"start":{"line":18,"column":8,"offset":1070},"end":{"line":18,"column":42,"offset":1104}}}],"position":{"start":{"line":18,"column":7,"offset":1069},"end":{"line":18,"column":43,"offset":1105}}}],"position":{"start":{"line":18,"column":3,"offset":1065},"end":{"line":18,"column":43,"offset":1105}}},{"type":"text","value":"\n"}],"position":{"start":{"line":18,"column":1,"offset":1063},"end":{"line":18,"column":43,"offset":1105}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":20,"column":1,"offset":1107},"end":{"line":20,"column":7,"offset":1113}}},{"type":"raw","value":"<br />","position":{"start":{"line":20,"column":7,"offset":1113},"end":{"line":20,"column":13,"offset":1119}}},{"type":"text","value":" ","position":{"start":{"line":20,"column":13,"offset":1119},"end":{"line":20,"column":19,"offset":1125}}},{"type":"text","value":" ","position":{"start":{"line":20,"column":19,"offset":1125},"end":{"line":20,"column":25,"offset":1131}}},{"type":"text","value":" ","position":{"start":{"line":20,"column":25,"offset":1131},"end":{"line":20,"column":31,"offset":1137}}},{"type":"text","value":" ","position":{"start":{"line":20,"column":31,"offset":1137},"end":{"line":20,"column":37,"offset":1143}}},{"type":"text","value":" ","position":{"start":{"line":20,"column":37,"offset":1143},"end":{"line":20,"column":43,"offset":1149}}},{"type":"text","value":" Transformer model consists of ","position":{"start":{"line":20,"column":43,"offset":1149},"end":{"line":20,"column":74,"offset":1180}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Encoder","position":{"start":{"line":20,"column":75,"offset":1181},"end":{"line":20,"column":82,"offset":1188}}}],"position":{"start":{"line":20,"column":74,"offset":1180},"end":{"line":20,"column":83,"offset":1189}}},{"type":"text","value":" layer and ","position":{"start":{"line":20,"column":83,"offset":1189},"end":{"line":20,"column":94,"offset":1200}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Decoder","position":{"start":{"line":20,"column":95,"offset":1201},"end":{"line":20,"column":102,"offset":1208}}}],"position":{"start":{"line":20,"column":94,"offset":1200},"end":{"line":20,"column":103,"offset":1209}}},{"type":"text","value":" layer, which is same architecture with ","position":{"start":{"line":20,"column":103,"offset":1209},"end":{"line":20,"column":143,"offset":1249}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"seq2seq","position":{"start":{"line":20,"column":145,"offset":1251},"end":{"line":20,"column":152,"offset":1258}}}],"position":{"start":{"line":20,"column":143,"offset":1249},"end":{"line":20,"column":154,"offset":1260}}},{"type":"text","value":" model, but the inside of each layer has ","position":{"start":{"line":20,"column":154,"offset":1260},"end":{"line":20,"column":195,"offset":1301}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"attention","position":{"start":{"line":20,"column":197,"offset":1303},"end":{"line":20,"column":206,"offset":1312}}}],"position":{"start":{"line":20,"column":195,"offset":1301},"end":{"line":20,"column":208,"offset":1314}}},{"type":"text","value":" layer not ","position":{"start":{"line":20,"column":208,"offset":1314},"end":{"line":20,"column":219,"offset":1325}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"RNN","position":{"start":{"line":20,"column":221,"offset":1327},"end":{"line":20,"column":224,"offset":1330}}}],"position":{"start":{"line":20,"column":219,"offset":1325},"end":{"line":20,"column":226,"offset":1332}}},{"type":"text","value":". The most important part of attention in Transforme model is ","position":{"start":{"line":20,"column":226,"offset":1332},"end":{"line":20,"column":288,"offset":1394}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Self-Attention","position":{"start":{"line":20,"column":290,"offset":1396},"end":{"line":20,"column":304,"offset":1410}}}],"position":{"start":{"line":20,"column":288,"offset":1394},"end":{"line":20,"column":306,"offset":1412}}},{"type":"text","value":".","position":{"start":{"line":20,"column":306,"offset":1412},"end":{"line":20,"column":307,"offset":1413}}}],"position":{"start":{"line":20,"column":1,"offset":1107},"end":{"line":20,"column":307,"offset":1413}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":22,"column":1,"offset":1415},"end":{"line":22,"column":7,"offset":1421}}},{"type":"text","value":" Self-Attention directly makes Transformer model see relationships between all words in sentence and figured it out which word to attend importantly. This mechanism is done through ","position":{"start":{"line":22,"column":7,"offset":1421},"end":{"line":22,"column":188,"offset":1602}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Encoder","position":{"start":{"line":22,"column":189,"offset":1603},"end":{"line":22,"column":196,"offset":1610}}}],"position":{"start":{"line":22,"column":188,"offset":1602},"end":{"line":22,"column":197,"offset":1611}}},{"type":"text","value":" and ","position":{"start":{"line":22,"column":197,"offset":1611},"end":{"line":22,"column":202,"offset":1616}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Decoder","position":{"start":{"line":22,"column":203,"offset":1617},"end":{"line":22,"column":210,"offset":1624}}}],"position":{"start":{"line":22,"column":202,"offset":1616},"end":{"line":22,"column":211,"offset":1625}}},{"type":"text","value":" layers. Interesting point is, the output of Encoder goes to Decoder layer and use it for the attention to get the relationship between the input and output for training. Illustration of this process would look like this..!😆","position":{"start":{"line":22,"column":211,"offset":1625},"end":{"line":22,"column":436,"offset":1850}}}],"position":{"start":{"line":22,"column":1,"offset":1415},"end":{"line":22,"column":436,"offset":1850}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1852},"end":{"line":24,"column":7,"offset":1858}}},{"type":"raw","value":"<img src=\"https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif\" width=\"600\" height=\"400\">","position":{"start":{"line":24,"column":7,"offset":1858},"end":{"line":24,"column":172,"offset":2023}}}],"position":{"start":{"line":24,"column":1,"offset":1852},"end":{"line":24,"column":172,"offset":2023}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h5","properties":{"id":"from-google-ai-blog","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#from-google-ai-blog","aria-label":"from google ai blog permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"From: Google AI Blog","position":{"start":{"line":26,"column":7,"offset":2031},"end":{"line":26,"column":27,"offset":2051}}}],"position":{"start":{"line":26,"column":1,"offset":2025},"end":{"line":26,"column":27,"offset":2051}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":28,"column":1,"offset":2053},"end":{"line":28,"column":7,"offset":2059}}},{"type":"text","value":" Let’s add more explanation for the mechanism. First, all words in sentence do self-attention and get output which has meaningful relationshop among words. After encoding, target word from Decoder do self-attention, but here comes important feature for Transformer comes in. As we can see from the illustration, Decoder of Transformer sees the part of the target word, not the entire sentence. This technique is called ","position":{"start":{"line":28,"column":7,"offset":2059},"end":{"line":28,"column":426,"offset":2478}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Masked-Self-Attention","position":{"start":{"line":28,"column":429,"offset":2481},"end":{"line":28,"column":450,"offset":2502}}}],"position":{"start":{"line":28,"column":428,"offset":2480},"end":{"line":28,"column":451,"offset":2503}}}],"position":{"start":{"line":28,"column":426,"offset":2478},"end":{"line":28,"column":453,"offset":2505}}},{"type":"text","value":". Because of this technique, Transformer model predicts ","position":{"start":{"line":28,"column":453,"offset":2505},"end":{"line":28,"column":509,"offset":2561}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":510,"offset":2562},"end":{"line":28,"column":511,"offset":2563}}}],"position":{"start":{"line":28,"column":509,"offset":2561},"end":{"line":28,"column":512,"offset":2564}}},{"type":"text","value":" depends only on the known output at position less than ","position":{"start":{"line":28,"column":512,"offset":2564},"end":{"line":28,"column":568,"offset":2620}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":569,"offset":2621},"end":{"line":28,"column":570,"offset":2622}}}],"position":{"start":{"line":28,"column":568,"offset":2620},"end":{"line":28,"column":571,"offset":2623}}},{"type":"text","value":". After completing masked-self-attention, we are doing another attention between the output of masked-self-attention and the output of Encoder.","position":{"start":{"line":28,"column":571,"offset":2623},"end":{"line":28,"column":714,"offset":2766}}}],"position":{"start":{"line":28,"column":1,"offset":2053},"end":{"line":28,"column":714,"offset":2766}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"conclusion","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#conclusion","aria-label":"conclusion permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Conclusion","position":{"start":{"line":30,"column":4,"offset":2771},"end":{"line":30,"column":14,"offset":2781}}}],"position":{"start":{"line":30,"column":1,"offset":2768},"end":{"line":30,"column":14,"offset":2781}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":" ","position":{"start":{"line":32,"column":1,"offset":2783},"end":{"line":32,"column":7,"offset":2789}}},{"type":"text","value":" ","position":{"start":{"line":32,"column":7,"offset":2789},"end":{"line":32,"column":13,"offset":2795}}},{"type":"text","value":" ","position":{"start":{"line":32,"column":13,"offset":2795},"end":{"line":32,"column":19,"offset":2801}}},{"type":"text","value":" ","position":{"start":{"line":32,"column":19,"offset":2801},"end":{"line":32,"column":25,"offset":2807}}},{"type":"text","value":" ","position":{"start":{"line":32,"column":25,"offset":2807},"end":{"line":32,"column":31,"offset":2813}}},{"type":"text","value":"Transformer model only uses ","position":{"start":{"line":32,"column":31,"offset":2813},"end":{"line":32,"column":59,"offset":2841}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Self Attention","position":{"start":{"line":32,"column":61,"offset":2843},"end":{"line":32,"column":75,"offset":2857}}}],"position":{"start":{"line":32,"column":59,"offset":2841},"end":{"line":32,"column":77,"offset":2859}}},{"type":"text","value":" for the language model and proves ","position":{"start":{"line":32,"column":77,"offset":2859},"end":{"line":32,"column":112,"offset":2894}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Attention is all you Need","position":{"start":{"line":32,"column":115,"offset":2897},"end":{"line":32,"column":140,"offset":2922}}}],"position":{"start":{"line":32,"column":114,"offset":2896},"end":{"line":32,"column":141,"offset":2923}}}],"position":{"start":{"line":32,"column":112,"offset":2894},"end":{"line":32,"column":143,"offset":2925}}},{"type":"text","value":" statement. Now, it became fundamental basis of NLP model. It would be excited to see the various application which is built based on Transformer model.","position":{"start":{"line":32,"column":143,"offset":2925},"end":{"line":32,"column":295,"offset":3077}}}],"position":{"start":{"line":32,"column":1,"offset":2783},"end":{"line":32,"column":295,"offset":3077}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":33,"column":1,"offset":3078}}}}