{"expireTime":9007200906038602000,"key":"transformer-remark-markdown-ast-e9738528e1609b5507701199e46c15cd-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7},"indent":[]}},{"type":"html","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz6WRX0/CMBTF+f5fQh/gnSiDESHCg9FgjAlGJeFPhKVra2xj1rK1dm3tcOjCQAKcNGlzkt/tvedW7Amq5LcxxmbnKDgvkdFmXW1Dv2YJ/pKCz5dqmTexWxuwkdrGIaWgWX+eMG6HT8P5IkgSEceJE2NsNnvjnGutx5MpRFgI8Qcn2kaLDxq2a4/TAPDB/aB73b/0Wn670/KvGp5/dl6tX3ghRM65ub1j7odVCxmcWqto9In6DyBwLsIYAIjxO0QIQtTp9l5eR4SQKGJSSkJomqalwLSw29JWSm0duLiqFb6O1I33E6x7FJMvB1ZY1L54/93zgfoGgyp+WM2mC3AAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"google\" title=\"google\" src=\"/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png\" srcset=\"/static/fbc6dd92a8903039c6d1223bd49971b7/5a46d/google.png 300w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/0a47e/google.png 600w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png 1200w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"link","url":"#introduction","title":null,"children":[],"data":{"hProperties":{"aria-label":"introduction permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Introduction","position":{"start":{"line":4,"column":4,"offset":68},"end":{"line":4,"column":16,"offset":80},"indent":[]}}],"position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":4,"column":16,"offset":80},"indent":[]},"data":{"id":"introduction","htmlAttributes":{"id":"introduction"},"hProperties":{"id":"introduction","style":"position:relative;"}}},{"type":"html","value":"<br />","position":{"start":{"line":6,"column":1,"offset":82},"end":{"line":6,"column":7,"offset":88},"indent":[]}},{"type":"heading","depth":3,"children":[{"type":"link","url":"#transformer-model-attention-is-all-you-need","title":null,"children":[],"data":{"hProperties":{"aria-label":"transformer model attention is all you need permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"emphasis","children":[{"type":"text","value":"Transformer Model: Attention is all you Need","position":{"start":{"line":8,"column":6,"offset":95},"end":{"line":8,"column":50,"offset":139},"indent":[]}}],"position":{"start":{"line":8,"column":5,"offset":94},"end":{"line":8,"column":51,"offset":140},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":90},"end":{"line":8,"column":51,"offset":140},"indent":[]},"data":{"id":"transformer-model-attention-is-all-you-need","htmlAttributes":{"id":"transformer-model-attention-is-all-you-need"},"hProperties":{"id":"transformer-model-attention-is-all-you-need","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":10,"column":1,"offset":142},"end":{"line":10,"column":7,"offset":148},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":10,"column":7,"offset":148},"end":{"line":10,"column":13,"offset":154},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":13,"offset":154},"end":{"line":10,"column":19,"offset":160},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":19,"offset":160},"end":{"line":10,"column":25,"offset":166},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":25,"offset":166},"end":{"line":10,"column":31,"offset":172},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":31,"offset":172},"end":{"line":10,"column":37,"offset":178},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":37,"offset":178},"end":{"line":10,"column":43,"offset":184},"indent":[]}},{"type":"text","value":"Transformer model was published by ","position":{"start":{"line":10,"column":43,"offset":184},"end":{"line":10,"column":78,"offset":219},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Google AI","position":{"start":{"line":10,"column":80,"offset":221},"end":{"line":10,"column":89,"offset":230},"indent":[]}}],"position":{"start":{"line":10,"column":78,"offset":219},"end":{"line":10,"column":91,"offset":232},"indent":[]}},{"type":"text","value":" on 2017, and became fundamental of Natural Language Process Model for the other models such as ","position":{"start":{"line":10,"column":91,"offset":232},"end":{"line":10,"column":187,"offset":328},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"BERT","position":{"start":{"line":10,"column":188,"offset":329},"end":{"line":10,"column":192,"offset":333},"indent":[]}}],"position":{"start":{"line":10,"column":187,"offset":328},"end":{"line":10,"column":193,"offset":334},"indent":[]}},{"type":"text","value":" or ","position":{"start":{"line":10,"column":193,"offset":334},"end":{"line":10,"column":197,"offset":338},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":10,"column":198,"offset":339},"end":{"line":10,"column":201,"offset":342},"indent":[]}}],"position":{"start":{"line":10,"column":197,"offset":338},"end":{"line":10,"column":202,"offset":343},"indent":[]}},{"type":"text","value":". What makes Transformer model attractive is, this model never uses ","position":{"start":{"line":10,"column":202,"offset":343},"end":{"line":10,"column":270,"offset":411},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"RNN","position":{"start":{"line":10,"column":272,"offset":413},"end":{"line":10,"column":275,"offset":416},"indent":[]}}],"position":{"start":{"line":10,"column":270,"offset":411},"end":{"line":10,"column":277,"offset":418},"indent":[]}},{"type":"text","value":", one of the greatest sequence model, architecture. It only uses ","position":{"start":{"line":10,"column":277,"offset":418},"end":{"line":10,"column":342,"offset":483},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Attention","position":{"start":{"line":10,"column":344,"offset":485},"end":{"line":10,"column":353,"offset":494},"indent":[]}}],"position":{"start":{"line":10,"column":342,"offset":483},"end":{"line":10,"column":355,"offset":496},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":10,"column":355,"offset":496},"end":{"line":10,"column":356,"offset":497},"indent":[]}}],"position":{"start":{"line":10,"column":1,"offset":142},"end":{"line":10,"column":356,"offset":497},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":12,"column":1,"offset":499},"end":{"line":12,"column":7,"offset":505},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":7,"offset":505},"end":{"line":12,"column":13,"offset":511},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":13,"offset":511},"end":{"line":12,"column":19,"offset":517},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":19,"offset":517},"end":{"line":12,"column":25,"offset":523},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":25,"offset":523},"end":{"line":12,"column":31,"offset":529},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":31,"offset":529},"end":{"line":12,"column":37,"offset":535},"indent":[]}},{"type":"text","value":"Our language has meaningful relationship in sentence, which we called ","position":{"start":{"line":12,"column":37,"offset":535},"end":{"line":12,"column":107,"offset":605},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"context","position":{"start":{"line":12,"column":108,"offset":606},"end":{"line":12,"column":115,"offset":613},"indent":[]}}],"position":{"start":{"line":12,"column":107,"offset":605},"end":{"line":12,"column":116,"offset":614},"indent":[]}},{"type":"text","value":". For human, we can easily guess what ","position":{"start":{"line":12,"column":116,"offset":614},"end":{"line":12,"column":154,"offset":652},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":155,"offset":653},"end":{"line":12,"column":157,"offset":655},"indent":[]}}],"position":{"start":{"line":12,"column":154,"offset":652},"end":{"line":12,"column":158,"offset":656},"indent":[]}},{"type":"text","value":" stands for, but for machine learning points of view, it is hard to train what ","position":{"start":{"line":12,"column":158,"offset":656},"end":{"line":12,"column":237,"offset":735},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":238,"offset":736},"end":{"line":12,"column":240,"offset":738},"indent":[]}}],"position":{"start":{"line":12,"column":237,"offset":735},"end":{"line":12,"column":241,"offset":739},"indent":[]}},{"type":"text","value":" means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where ","position":{"start":{"line":12,"column":241,"offset":739},"end":{"line":12,"column":465,"offset":963},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Attention","position":{"start":{"line":12,"column":467,"offset":965},"end":{"line":12,"column":476,"offset":974},"indent":[]}}],"position":{"start":{"line":12,"column":465,"offset":963},"end":{"line":12,"column":478,"offset":976},"indent":[]}},{"type":"text","value":" shines, and ","position":{"start":{"line":12,"column":478,"offset":976},"end":{"line":12,"column":491,"offset":989},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Transformer","position":{"start":{"line":12,"column":493,"offset":991},"end":{"line":12,"column":504,"offset":1002},"indent":[]}}],"position":{"start":{"line":12,"column":491,"offset":989},"end":{"line":12,"column":506,"offset":1004},"indent":[]}},{"type":"text","value":" model only uses this technique.","position":{"start":{"line":12,"column":506,"offset":1004},"end":{"line":12,"column":538,"offset":1036},"indent":[]}}],"position":{"start":{"line":12,"column":1,"offset":499},"end":{"line":12,"column":538,"offset":1036},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"link","url":"#architecture","title":null,"children":[],"data":{"hProperties":{"aria-label":"architecture permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Architecture","position":{"start":{"line":14,"column":4,"offset":1041},"end":{"line":14,"column":16,"offset":1053},"indent":[]}}],"position":{"start":{"line":14,"column":1,"offset":1038},"end":{"line":14,"column":16,"offset":1053},"indent":[]},"data":{"id":"architecture","htmlAttributes":{"id":"architecture"},"hProperties":{"id":"architecture","style":"position:relative;"}}},{"type":"html","value":"<br />","position":{"start":{"line":16,"column":1,"offset":1055},"end":{"line":16,"column":7,"offset":1061},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#input---encoder---deocder---output","title":null,"children":[],"data":{"hProperties":{"aria-label":"input   encoder   deocder   output permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"emphasis","children":[{"type":"text","value":"Input - Encoder - Deocder - Output","position":{"start":{"line":18,"column":8,"offset":1070},"end":{"line":18,"column":42,"offset":1104},"indent":[]}}],"position":{"start":{"line":18,"column":7,"offset":1069},"end":{"line":18,"column":43,"offset":1105},"indent":[]}}],"position":{"start":{"line":18,"column":3,"offset":1065},"end":{"line":18,"column":43,"offset":1105},"indent":[]},"data":{"id":"input---encoder---deocder---output","htmlAttributes":{"id":"input---encoder---deocder---output"},"hProperties":{"id":"input---encoder---deocder---output","style":"position:relative;"}}}],"position":{"start":{"line":18,"column":1,"offset":1063},"end":{"line":18,"column":43,"offset":1105},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":20,"column":1,"offset":1107},"end":{"line":20,"column":7,"offset":1113},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":20,"column":7,"offset":1113},"end":{"line":20,"column":13,"offset":1119},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":13,"offset":1119},"end":{"line":20,"column":19,"offset":1125},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":19,"offset":1125},"end":{"line":20,"column":25,"offset":1131},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":25,"offset":1131},"end":{"line":20,"column":31,"offset":1137},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":31,"offset":1137},"end":{"line":20,"column":37,"offset":1143},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":37,"offset":1143},"end":{"line":20,"column":43,"offset":1149},"indent":[]}},{"type":"text","value":" Transformer model consists of ","position":{"start":{"line":20,"column":43,"offset":1149},"end":{"line":20,"column":74,"offset":1180},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Encoder","position":{"start":{"line":20,"column":75,"offset":1181},"end":{"line":20,"column":82,"offset":1188},"indent":[]}}],"position":{"start":{"line":20,"column":74,"offset":1180},"end":{"line":20,"column":83,"offset":1189},"indent":[]}},{"type":"text","value":" layer and ","position":{"start":{"line":20,"column":83,"offset":1189},"end":{"line":20,"column":94,"offset":1200},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Decoder","position":{"start":{"line":20,"column":95,"offset":1201},"end":{"line":20,"column":102,"offset":1208},"indent":[]}}],"position":{"start":{"line":20,"column":94,"offset":1200},"end":{"line":20,"column":103,"offset":1209},"indent":[]}},{"type":"text","value":" layer, which is same architecture with ","position":{"start":{"line":20,"column":103,"offset":1209},"end":{"line":20,"column":143,"offset":1249},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"seq2seq","position":{"start":{"line":20,"column":145,"offset":1251},"end":{"line":20,"column":152,"offset":1258},"indent":[]}}],"position":{"start":{"line":20,"column":143,"offset":1249},"end":{"line":20,"column":154,"offset":1260},"indent":[]}},{"type":"text","value":" model, but the inside of each layer has ","position":{"start":{"line":20,"column":154,"offset":1260},"end":{"line":20,"column":195,"offset":1301},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"attention","position":{"start":{"line":20,"column":197,"offset":1303},"end":{"line":20,"column":206,"offset":1312},"indent":[]}}],"position":{"start":{"line":20,"column":195,"offset":1301},"end":{"line":20,"column":208,"offset":1314},"indent":[]}},{"type":"text","value":" layer not ","position":{"start":{"line":20,"column":208,"offset":1314},"end":{"line":20,"column":219,"offset":1325},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"RNN","position":{"start":{"line":20,"column":221,"offset":1327},"end":{"line":20,"column":224,"offset":1330},"indent":[]}}],"position":{"start":{"line":20,"column":219,"offset":1325},"end":{"line":20,"column":226,"offset":1332},"indent":[]}},{"type":"text","value":". The most important part of attention in Transforme model is ","position":{"start":{"line":20,"column":226,"offset":1332},"end":{"line":20,"column":288,"offset":1394},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Self-Attention","position":{"start":{"line":20,"column":290,"offset":1396},"end":{"line":20,"column":304,"offset":1410},"indent":[]}}],"position":{"start":{"line":20,"column":288,"offset":1394},"end":{"line":20,"column":306,"offset":1412},"indent":[]}},{"type":"text","value":".","position":{"start":{"line":20,"column":306,"offset":1412},"end":{"line":20,"column":307,"offset":1413},"indent":[]}}],"position":{"start":{"line":20,"column":1,"offset":1107},"end":{"line":20,"column":307,"offset":1413},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":22,"column":1,"offset":1415},"end":{"line":22,"column":7,"offset":1421},"indent":[]}},{"type":"text","value":" Self-Attention directly makes Transformer model see relationships between all words in sentence and figured it out which word to attend importantly. This mechanism is done through ","position":{"start":{"line":22,"column":7,"offset":1421},"end":{"line":22,"column":188,"offset":1602},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Encoder","position":{"start":{"line":22,"column":189,"offset":1603},"end":{"line":22,"column":196,"offset":1610},"indent":[]}}],"position":{"start":{"line":22,"column":188,"offset":1602},"end":{"line":22,"column":197,"offset":1611},"indent":[]}},{"type":"text","value":" and ","position":{"start":{"line":22,"column":197,"offset":1611},"end":{"line":22,"column":202,"offset":1616},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Decoder","position":{"start":{"line":22,"column":203,"offset":1617},"end":{"line":22,"column":210,"offset":1624},"indent":[]}}],"position":{"start":{"line":22,"column":202,"offset":1616},"end":{"line":22,"column":211,"offset":1625},"indent":[]}},{"type":"text","value":" layers. Interesting point is, the output of Encoder goes to Decoder layer and use it for the attention to get the relationship between the input and output for training. Illustration of this process would look like this..!ðŸ˜†","position":{"start":{"line":22,"column":211,"offset":1625},"end":{"line":22,"column":436,"offset":1850},"indent":[]}}],"position":{"start":{"line":22,"column":1,"offset":1415},"end":{"line":22,"column":436,"offset":1850},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1852},"end":{"line":24,"column":7,"offset":1858},"indent":[]}},{"type":"html","value":"<img src=\"https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif\" width=\"600\" height=\"400\">","position":{"start":{"line":24,"column":7,"offset":1858},"end":{"line":24,"column":172,"offset":2023},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":1852},"end":{"line":24,"column":172,"offset":2023},"indent":[]}},{"type":"heading","depth":5,"children":[{"type":"link","url":"#from-google-ai-blog","title":null,"children":[],"data":{"hProperties":{"aria-label":"from google ai blog permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"From: Google AI Blog","position":{"start":{"line":26,"column":7,"offset":2031},"end":{"line":26,"column":27,"offset":2051},"indent":[]}}],"position":{"start":{"line":26,"column":1,"offset":2025},"end":{"line":26,"column":27,"offset":2051},"indent":[]},"data":{"id":"from-google-ai-blog","htmlAttributes":{"id":"from-google-ai-blog"},"hProperties":{"id":"from-google-ai-blog","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":28,"column":1,"offset":2053},"end":{"line":28,"column":7,"offset":2059},"indent":[]}},{"type":"text","value":" Letâ€™s add more explanation for the mechanism. First, all words in sentence do self-attention and get output which has meaningful relationshop among words. After encoding, target word from Decoder do self-attention, but here comes important feature for Transformer comes in. As we can see from the illustration, Decoder of Transformer sees the part of the target word, not the entire sentence. This technique is called ","position":{"start":{"line":28,"column":7,"offset":2059},"end":{"line":28,"column":426,"offset":2478},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"Masked-Self-Attention","position":{"start":{"line":28,"column":429,"offset":2481},"end":{"line":28,"column":450,"offset":2502},"indent":[]}}],"position":{"start":{"line":28,"column":428,"offset":2480},"end":{"line":28,"column":451,"offset":2503},"indent":[]}}],"position":{"start":{"line":28,"column":426,"offset":2478},"end":{"line":28,"column":453,"offset":2505},"indent":[]}},{"type":"text","value":". Because of this technique, Transformer model predicts ","position":{"start":{"line":28,"column":453,"offset":2505},"end":{"line":28,"column":509,"offset":2561},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":510,"offset":2562},"end":{"line":28,"column":511,"offset":2563},"indent":[]}}],"position":{"start":{"line":28,"column":509,"offset":2561},"end":{"line":28,"column":512,"offset":2564},"indent":[]}},{"type":"text","value":" depends only on the known output at position less than ","position":{"start":{"line":28,"column":512,"offset":2564},"end":{"line":28,"column":568,"offset":2620},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":569,"offset":2621},"end":{"line":28,"column":570,"offset":2622},"indent":[]}}],"position":{"start":{"line":28,"column":568,"offset":2620},"end":{"line":28,"column":571,"offset":2623},"indent":[]}},{"type":"text","value":". After completing masked-self-attention, we are doing another attention between the output of masked-self-attention and the output of Encoder.","position":{"start":{"line":28,"column":571,"offset":2623},"end":{"line":28,"column":714,"offset":2766},"indent":[]}}],"position":{"start":{"line":28,"column":1,"offset":2053},"end":{"line":28,"column":714,"offset":2766},"indent":[]}},{"type":"heading","depth":2,"children":[{"type":"link","url":"#conclusion","title":null,"children":[],"data":{"hProperties":{"aria-label":"conclusion permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Conclusion","position":{"start":{"line":30,"column":4,"offset":2771},"end":{"line":30,"column":14,"offset":2781},"indent":[]}}],"position":{"start":{"line":30,"column":1,"offset":2768},"end":{"line":30,"column":14,"offset":2781},"indent":[]},"data":{"id":"conclusion","htmlAttributes":{"id":"conclusion"},"hProperties":{"id":"conclusion","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"Â ","position":{"start":{"line":32,"column":1,"offset":2783},"end":{"line":32,"column":7,"offset":2789},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":7,"offset":2789},"end":{"line":32,"column":13,"offset":2795},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":13,"offset":2795},"end":{"line":32,"column":19,"offset":2801},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":19,"offset":2801},"end":{"line":32,"column":25,"offset":2807},"indent":[]}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":25,"offset":2807},"end":{"line":32,"column":31,"offset":2813},"indent":[]}},{"type":"text","value":"Transformer model only uses ","position":{"start":{"line":32,"column":31,"offset":2813},"end":{"line":32,"column":59,"offset":2841},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Self Attention","position":{"start":{"line":32,"column":61,"offset":2843},"end":{"line":32,"column":75,"offset":2857},"indent":[]}}],"position":{"start":{"line":32,"column":59,"offset":2841},"end":{"line":32,"column":77,"offset":2859},"indent":[]}},{"type":"text","value":" for the language model and proves ","position":{"start":{"line":32,"column":77,"offset":2859},"end":{"line":32,"column":112,"offset":2894},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"Attention is all you Need","position":{"start":{"line":32,"column":115,"offset":2897},"end":{"line":32,"column":140,"offset":2922},"indent":[]}}],"position":{"start":{"line":32,"column":114,"offset":2896},"end":{"line":32,"column":141,"offset":2923},"indent":[]}}],"position":{"start":{"line":32,"column":112,"offset":2894},"end":{"line":32,"column":143,"offset":2925},"indent":[]}},{"type":"text","value":" statement. Now, it became fundamental basis of NLP model. It would be excited to see the various application which is built based on Transformer model.","position":{"start":{"line":32,"column":143,"offset":2925},"end":{"line":32,"column":295,"offset":3077},"indent":[]}}],"position":{"start":{"line":32,"column":1,"offset":2783},"end":{"line":32,"column":295,"offset":3077},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":33,"column":1,"offset":3078}}}}