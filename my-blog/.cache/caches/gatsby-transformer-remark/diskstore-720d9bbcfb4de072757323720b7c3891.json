{"expireTime":9007200905817935000,"key":"transformer-remark-markdown-html-ast-eb3359e147d69cb0d43edde2733dfe97-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7}}},{"type":"raw","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz6WRX0/CMBTF+f5fQh/gnSiDESHCg9FgjAlGJeFPhKVra2xj1rK1dm3tcOjCQAKcNGlzkt/tvedW7Amq5LcxxmbnKDgvkdFmXW1Dv2YJ/pKCz5dqmTexWxuwkdrGIaWgWX+eMG6HT8P5IkgSEceJE2NsNnvjnGutx5MpRFgI8Qcn2kaLDxq2a4/TAPDB/aB73b/0Wn670/KvGp5/dl6tX3ghRM65ub1j7odVCxmcWqto9In6DyBwLsIYAIjxO0QIQtTp9l5eR4SQKGJSSkJomqalwLSw29JWSm0duLiqFb6O1I33E6x7FJMvB1ZY1L54/93zgfoGgyp+WM2mC3AAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"google\" title=\"google\" src=\"/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png\" srcset=\"/static/fbc6dd92a8903039c6d1223bd49971b7/5a46d/google.png 300w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/0a47e/google.png 600w,\n/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png 1200w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63}}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"introduction","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#introduction","aria-label":"introduction permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Introduction","position":{"start":{"line":4,"column":4,"offset":68},"end":{"line":4,"column":16,"offset":80}}}],"position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":4,"column":16,"offset":80}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":6,"column":1,"offset":82},"end":{"line":6,"column":7,"offset":88}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"transformer-model-attention-is-all-you-need","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#transformer-model-attention-is-all-you-need","aria-label":"transformer model attention is all you need permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Transformer Model: Attention is all you Need","position":{"start":{"line":8,"column":8,"offset":97},"end":{"line":8,"column":52,"offset":141}}}],"position":{"start":{"line":8,"column":7,"offset":96},"end":{"line":8,"column":53,"offset":142}}}],"position":{"start":{"line":8,"column":3,"offset":92},"end":{"line":8,"column":53,"offset":142}}},{"type":"text","value":"\n"}],"position":{"start":{"line":8,"column":1,"offset":90},"end":{"line":8,"column":53,"offset":142}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":1,"offset":144},"end":{"line":10,"column":7,"offset":150}}},{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":7,"offset":150},"end":{"line":10,"column":13,"offset":156}}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":13,"offset":156},"end":{"line":10,"column":19,"offset":162}}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":19,"offset":162},"end":{"line":10,"column":25,"offset":168}}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":25,"offset":168},"end":{"line":10,"column":31,"offset":174}}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":31,"offset":174},"end":{"line":10,"column":37,"offset":180}}},{"type":"text","value":"Â ","position":{"start":{"line":10,"column":37,"offset":180},"end":{"line":10,"column":43,"offset":186}}},{"type":"text","value":"Transformer model was published by ","position":{"start":{"line":10,"column":43,"offset":186},"end":{"line":10,"column":78,"offset":221}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Google AI","position":{"start":{"line":10,"column":80,"offset":223},"end":{"line":10,"column":89,"offset":232}}}],"position":{"start":{"line":10,"column":78,"offset":221},"end":{"line":10,"column":91,"offset":234}}},{"type":"text","value":" on 2017, and became fundamental of Natural Language Process Model for the other models such as ","position":{"start":{"line":10,"column":91,"offset":234},"end":{"line":10,"column":187,"offset":330}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"BERT","position":{"start":{"line":10,"column":188,"offset":331},"end":{"line":10,"column":192,"offset":335}}}],"position":{"start":{"line":10,"column":187,"offset":330},"end":{"line":10,"column":193,"offset":336}}},{"type":"text","value":" or ","position":{"start":{"line":10,"column":193,"offset":336},"end":{"line":10,"column":197,"offset":340}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"GPT","position":{"start":{"line":10,"column":198,"offset":341},"end":{"line":10,"column":201,"offset":344}}}],"position":{"start":{"line":10,"column":197,"offset":340},"end":{"line":10,"column":202,"offset":345}}},{"type":"text","value":". What makes Transformer model attractive is, this model never uses ","position":{"start":{"line":10,"column":202,"offset":345},"end":{"line":10,"column":270,"offset":413}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"RNN","position":{"start":{"line":10,"column":272,"offset":415},"end":{"line":10,"column":275,"offset":418}}}],"position":{"start":{"line":10,"column":270,"offset":413},"end":{"line":10,"column":277,"offset":420}}},{"type":"text","value":", one of the greatest sequence model, architecture. It only uses ","position":{"start":{"line":10,"column":277,"offset":420},"end":{"line":10,"column":342,"offset":485}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Attention","position":{"start":{"line":10,"column":344,"offset":487},"end":{"line":10,"column":353,"offset":496}}}],"position":{"start":{"line":10,"column":342,"offset":485},"end":{"line":10,"column":355,"offset":498}}},{"type":"text","value":".","position":{"start":{"line":10,"column":355,"offset":498},"end":{"line":10,"column":356,"offset":499}}}],"position":{"start":{"line":10,"column":1,"offset":144},"end":{"line":10,"column":356,"offset":499}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":12,"column":1,"offset":501},"end":{"line":12,"column":7,"offset":507}}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":7,"offset":507},"end":{"line":12,"column":13,"offset":513}}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":13,"offset":513},"end":{"line":12,"column":19,"offset":519}}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":19,"offset":519},"end":{"line":12,"column":25,"offset":525}}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":25,"offset":525},"end":{"line":12,"column":31,"offset":531}}},{"type":"text","value":"Â ","position":{"start":{"line":12,"column":31,"offset":531},"end":{"line":12,"column":37,"offset":537}}},{"type":"text","value":"Our language has meaningful relationship in sentence, which we called ","position":{"start":{"line":12,"column":37,"offset":537},"end":{"line":12,"column":107,"offset":607}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"context","position":{"start":{"line":12,"column":108,"offset":608},"end":{"line":12,"column":115,"offset":615}}}],"position":{"start":{"line":12,"column":107,"offset":607},"end":{"line":12,"column":116,"offset":616}}},{"type":"text","value":". For human, we can easily guess what ","position":{"start":{"line":12,"column":116,"offset":616},"end":{"line":12,"column":154,"offset":654}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":155,"offset":655},"end":{"line":12,"column":157,"offset":657}}}],"position":{"start":{"line":12,"column":154,"offset":654},"end":{"line":12,"column":158,"offset":658}}},{"type":"text","value":" stands for, but for machine learning points of view, it is hard to train what ","position":{"start":{"line":12,"column":158,"offset":658},"end":{"line":12,"column":237,"offset":737}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"it","position":{"start":{"line":12,"column":238,"offset":738},"end":{"line":12,"column":240,"offset":740}}}],"position":{"start":{"line":12,"column":237,"offset":737},"end":{"line":12,"column":241,"offset":741}}},{"type":"text","value":" means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where ","position":{"start":{"line":12,"column":241,"offset":741},"end":{"line":12,"column":465,"offset":965}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Attention","position":{"start":{"line":12,"column":467,"offset":967},"end":{"line":12,"column":476,"offset":976}}}],"position":{"start":{"line":12,"column":465,"offset":965},"end":{"line":12,"column":478,"offset":978}}},{"type":"text","value":" shines, and ","position":{"start":{"line":12,"column":478,"offset":978},"end":{"line":12,"column":491,"offset":991}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Transformer","position":{"start":{"line":12,"column":493,"offset":993},"end":{"line":12,"column":504,"offset":1004}}}],"position":{"start":{"line":12,"column":491,"offset":991},"end":{"line":12,"column":506,"offset":1006}}},{"type":"text","value":" model only uses this technique.","position":{"start":{"line":12,"column":506,"offset":1006},"end":{"line":12,"column":538,"offset":1038}}}],"position":{"start":{"line":12,"column":1,"offset":501},"end":{"line":12,"column":538,"offset":1038}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"architecture","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#architecture","aria-label":"architecture permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Architecture","position":{"start":{"line":14,"column":4,"offset":1043},"end":{"line":14,"column":16,"offset":1055}}}],"position":{"start":{"line":14,"column":1,"offset":1040},"end":{"line":14,"column":16,"offset":1055}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":16,"column":1,"offset":1057},"end":{"line":16,"column":7,"offset":1063}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"input---encoder---deocder---output","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#input---encoder---deocder---output","aria-label":"input   encoder   deocder   output permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Input - Encoder - Deocder - Output","position":{"start":{"line":18,"column":8,"offset":1072},"end":{"line":18,"column":42,"offset":1106}}}],"position":{"start":{"line":18,"column":7,"offset":1071},"end":{"line":18,"column":43,"offset":1107}}}],"position":{"start":{"line":18,"column":3,"offset":1067},"end":{"line":18,"column":43,"offset":1107}}},{"type":"text","value":"\n"}],"position":{"start":{"line":18,"column":1,"offset":1065},"end":{"line":18,"column":43,"offset":1107}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":20,"column":1,"offset":1109},"end":{"line":20,"column":7,"offset":1115}}},{"type":"raw","value":"<br />","position":{"start":{"line":20,"column":7,"offset":1115},"end":{"line":20,"column":13,"offset":1121}}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":13,"offset":1121},"end":{"line":20,"column":19,"offset":1127}}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":19,"offset":1127},"end":{"line":20,"column":25,"offset":1133}}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":25,"offset":1133},"end":{"line":20,"column":31,"offset":1139}}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":31,"offset":1139},"end":{"line":20,"column":37,"offset":1145}}},{"type":"text","value":"Â ","position":{"start":{"line":20,"column":37,"offset":1145},"end":{"line":20,"column":43,"offset":1151}}},{"type":"text","value":" Transformer model consists of ","position":{"start":{"line":20,"column":43,"offset":1151},"end":{"line":20,"column":74,"offset":1182}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Encoder","position":{"start":{"line":20,"column":75,"offset":1183},"end":{"line":20,"column":82,"offset":1190}}}],"position":{"start":{"line":20,"column":74,"offset":1182},"end":{"line":20,"column":83,"offset":1191}}},{"type":"text","value":" layer and ","position":{"start":{"line":20,"column":83,"offset":1191},"end":{"line":20,"column":94,"offset":1202}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Decoder","position":{"start":{"line":20,"column":95,"offset":1203},"end":{"line":20,"column":102,"offset":1210}}}],"position":{"start":{"line":20,"column":94,"offset":1202},"end":{"line":20,"column":103,"offset":1211}}},{"type":"text","value":" layer, which is same architecture with ","position":{"start":{"line":20,"column":103,"offset":1211},"end":{"line":20,"column":143,"offset":1251}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"seq2seq","position":{"start":{"line":20,"column":145,"offset":1253},"end":{"line":20,"column":152,"offset":1260}}}],"position":{"start":{"line":20,"column":143,"offset":1251},"end":{"line":20,"column":154,"offset":1262}}},{"type":"text","value":" model, but the inside of each layer has ","position":{"start":{"line":20,"column":154,"offset":1262},"end":{"line":20,"column":195,"offset":1303}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"attention","position":{"start":{"line":20,"column":197,"offset":1305},"end":{"line":20,"column":206,"offset":1314}}}],"position":{"start":{"line":20,"column":195,"offset":1303},"end":{"line":20,"column":208,"offset":1316}}},{"type":"text","value":" layer not ","position":{"start":{"line":20,"column":208,"offset":1316},"end":{"line":20,"column":219,"offset":1327}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"RNN","position":{"start":{"line":20,"column":221,"offset":1329},"end":{"line":20,"column":224,"offset":1332}}}],"position":{"start":{"line":20,"column":219,"offset":1327},"end":{"line":20,"column":226,"offset":1334}}},{"type":"text","value":". The most important part of attention in Transforme model is ","position":{"start":{"line":20,"column":226,"offset":1334},"end":{"line":20,"column":288,"offset":1396}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Self-Attention","position":{"start":{"line":20,"column":290,"offset":1398},"end":{"line":20,"column":304,"offset":1412}}}],"position":{"start":{"line":20,"column":288,"offset":1396},"end":{"line":20,"column":306,"offset":1414}}},{"type":"text","value":".","position":{"start":{"line":20,"column":306,"offset":1414},"end":{"line":20,"column":307,"offset":1415}}}],"position":{"start":{"line":20,"column":1,"offset":1109},"end":{"line":20,"column":307,"offset":1415}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":22,"column":1,"offset":1417},"end":{"line":22,"column":7,"offset":1423}}},{"type":"text","value":" Self-Attention directly makes Transformer model see relationships between all words in sentence and figured it out which word to attend importantly. This mechanism is done through ","position":{"start":{"line":22,"column":7,"offset":1423},"end":{"line":22,"column":188,"offset":1604}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Encoder","position":{"start":{"line":22,"column":189,"offset":1605},"end":{"line":22,"column":196,"offset":1612}}}],"position":{"start":{"line":22,"column":188,"offset":1604},"end":{"line":22,"column":197,"offset":1613}}},{"type":"text","value":" and ","position":{"start":{"line":22,"column":197,"offset":1613},"end":{"line":22,"column":202,"offset":1618}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Decoder","position":{"start":{"line":22,"column":203,"offset":1619},"end":{"line":22,"column":210,"offset":1626}}}],"position":{"start":{"line":22,"column":202,"offset":1618},"end":{"line":22,"column":211,"offset":1627}}},{"type":"text","value":" layers. Interesting point is, the output of Encoder goes to Decoder layer and use it for the attention to get the relationship between the input and output for training. Illustration of this process would look like this..!ðŸ˜†","position":{"start":{"line":22,"column":211,"offset":1627},"end":{"line":22,"column":436,"offset":1852}}}],"position":{"start":{"line":22,"column":1,"offset":1417},"end":{"line":22,"column":436,"offset":1852}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1854},"end":{"line":24,"column":7,"offset":1860}}},{"type":"raw","value":"<img src=\"https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif\" width=\"600\" height=\"400\">","position":{"start":{"line":24,"column":7,"offset":1860},"end":{"line":24,"column":172,"offset":2025}}}],"position":{"start":{"line":24,"column":1,"offset":1854},"end":{"line":24,"column":172,"offset":2025}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h5","properties":{"id":"from-google-ai-blog","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#from-google-ai-blog","aria-label":"from google ai blog permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"From: Google AI Blog","position":{"start":{"line":26,"column":7,"offset":2033},"end":{"line":26,"column":27,"offset":2053}}}],"position":{"start":{"line":26,"column":1,"offset":2027},"end":{"line":26,"column":27,"offset":2053}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":28,"column":1,"offset":2055},"end":{"line":28,"column":7,"offset":2061}}},{"type":"text","value":" Letâ€™s add more explanation for the mechanism. First, all words in sentence do self-attention and get output which has meaningful relationshop among words. After encoding, target word from Decoder do self-attention, but here comes important feature for Transformer comes in. As we can see from the illustration, Decoder of Transformer sees the part of the target word, not the entire sentence. This technique is called ","position":{"start":{"line":28,"column":7,"offset":2061},"end":{"line":28,"column":426,"offset":2480}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Masked-Self-Attention","position":{"start":{"line":28,"column":429,"offset":2483},"end":{"line":28,"column":450,"offset":2504}}}],"position":{"start":{"line":28,"column":428,"offset":2482},"end":{"line":28,"column":451,"offset":2505}}}],"position":{"start":{"line":28,"column":426,"offset":2480},"end":{"line":28,"column":453,"offset":2507}}},{"type":"text","value":". Because of this technique, Transformer model predicts ","position":{"start":{"line":28,"column":453,"offset":2507},"end":{"line":28,"column":509,"offset":2563}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":510,"offset":2564},"end":{"line":28,"column":511,"offset":2565}}}],"position":{"start":{"line":28,"column":509,"offset":2563},"end":{"line":28,"column":512,"offset":2566}}},{"type":"text","value":" depends only on the known output at position less than ","position":{"start":{"line":28,"column":512,"offset":2566},"end":{"line":28,"column":568,"offset":2622}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"i","position":{"start":{"line":28,"column":569,"offset":2623},"end":{"line":28,"column":570,"offset":2624}}}],"position":{"start":{"line":28,"column":568,"offset":2622},"end":{"line":28,"column":571,"offset":2625}}},{"type":"text","value":" more output of the Decoder is feeding into the next target word, in this example, â€˜output of startâ€™ -> â€˜output of start and Jeâ€™ -> â€˜output of start and Je and suisâ€™ -> â€˜output of start and Je and suis -> arriveâ€™.","position":{"start":{"line":28,"column":571,"offset":2625},"end":{"line":28,"column":784,"offset":2838}}}],"position":{"start":{"line":28,"column":1,"offset":2055},"end":{"line":28,"column":784,"offset":2838}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"conclusion","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#conclusion","aria-label":"conclusion permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Conclusion","position":{"start":{"line":30,"column":4,"offset":2843},"end":{"line":30,"column":14,"offset":2853}}}],"position":{"start":{"line":30,"column":1,"offset":2840},"end":{"line":30,"column":14,"offset":2853}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Â ","position":{"start":{"line":32,"column":1,"offset":2855},"end":{"line":32,"column":7,"offset":2861}}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":7,"offset":2861},"end":{"line":32,"column":13,"offset":2867}}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":13,"offset":2867},"end":{"line":32,"column":19,"offset":2873}}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":19,"offset":2873},"end":{"line":32,"column":25,"offset":2879}}},{"type":"text","value":"Â ","position":{"start":{"line":32,"column":25,"offset":2879},"end":{"line":32,"column":31,"offset":2885}}},{"type":"text","value":"Transformer is beautiful!","position":{"start":{"line":32,"column":31,"offset":2885},"end":{"line":32,"column":56,"offset":2910}}}],"position":{"start":{"line":32,"column":1,"offset":2855},"end":{"line":32,"column":56,"offset":2910}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":33,"column":1,"offset":2911}}}}