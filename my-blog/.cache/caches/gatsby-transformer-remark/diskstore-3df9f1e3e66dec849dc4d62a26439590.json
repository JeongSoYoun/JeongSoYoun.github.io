{"expireTime":9007200906165288000,"key":"transformer-remark-markdown-html-1ec6471fd661cf01930bb258a7d3f793-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":"<p><br /><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAADEElEQVQozx2Si1PSBwCAf3/XdrdVt0e3re20dl5b2qlnE0UReU4FlfmIYfIwYCEIqIFgonCiTmjNlrHZiktNapooGD43a95NduZ94/YPfHfffZ/wxd0tSqaylIUyVPk2ELvXkdnXaDI+Q9f5GH3zA8yNcRzVUYZLQ4Qu+pn5xMsvH7h4fK6f1FkHmTM/cPC+jeMzNoSqyTQ1E5tI7mwh9+f4dnAbjTOHri9Nt36ZntYEFkUMhzjMcGWQ0FdDzF50Mn/BTvJ8Hy8+NJI9+z1/vtfF8TvtCJLRl0iDmf9hCm8OlXOH1v4duqwZDD0pTLoFbOo4LkkYnyhIuGKQ+JV+EiU2nhabWP1Mz6vz3/HXuWby78oQaj0FzYEMMncWR3QPe2iHDvsGHYbn9HQkuamZx6WcxScNExIHmK4eYq7KyUKljcUrvaxd7uZVUQuHn6vIf1SHcNWS4xtLlsEf9xiL7zIzt8vtwDoO+zLxiTVG9A8Za71LRBEhph7nvirIgmqIRc0Q6wY/GZ2T184Qh6VK8pdqEUo69hAZtgjF93EFNvGNppmZShOLrBF2PGH53jpLkymeR1dIx1Jszi6RmfiNrYl5Dh8+JWfychSM8Lqynn8rqhHKNPtc02Rx+beJzmwzXSg+OriC3/6Ee8FFEuNLpOIpkiMJ1qK/84cnTto9zV4wxoF7jDe3xzi+M84/9WJOROWFyvIDahqzqNSrWE2rOIzLdEp/pqdhBqt0nOGWwiqtI4TqbPzUaOORzMILhZEtmZ4DaRtHyibyigZO5Nd4K72MIBLtI6nZQFb9DGXVI7TV9+kSz9JbF8EqCTAg8eCX3mJSZWNO3UeyyczL5l52Wwy80XZyrG3mpK2Bt23lnLYVI9SX7iItW0V5NUlz+Txt5TE6K8IYqgLcFHlx1/cTlFmZUlt4oDWxqDOS7rzBXreBv/Vd5A1aTnqlnJoLQPOnCLKiHPKiFZTFv9J0KY7myzDtJT6ufz1AX8UtPLVWxgqaMY2ZRLeJZZORTfsN9p09HA3oybvbOfHIOfUWgJ6P+Q9k0XsmDcsekgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"openai\" title=\"openai\" src=\"/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png\" srcset=\"/static/7c1f9fb711eecac5f959f784796c2791/5a46d/openai.png 300w,\n/static/7c1f9fb711eecac5f959f784796c2791/0a47e/openai.png 600w,\n/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png 1200w,\n/static/7c1f9fb711eecac5f959f784796c2791/229ad/openai.png 1356w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span></p>\n<br />\n<blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<p>From Transformer model with <em>attention</em> , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about <strong><em>GPT</em></strong> model.</p>\n</blockquote>\n<p><br /><br />       GPT, <em>Generative Pre Training</em>, is a language model developed by <strong>Open AI</strong> with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).</p>\n<br />\n<blockquote>\n<h3 id=\"trainig-process\" style=\"position:relative;\"><a href=\"#trainig-process\" aria-label=\"trainig process permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Trainig Process</h3>\n<p>From pre-training to fine-tuning</p>\n</blockquote>\n<p><br />       First, pre-training(unsupervised learning with set of large-corpus of text data) is done to learn the parameters of language model. Since, the goal of unsupervised learning is to initialize the training, the objectives of learning remain same with supervised learning. After pre-training, supervised learning with same parameters that have pre-trained is processed with fine-tuning of some change of input tokens and the weight of the output layer.</p>\n<br />\n<blockquote>\n<h3 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h3>\n</blockquote>\n<p><br />       From pre-training to fine-tuning, architecture of GPT is <strong><em>Transformer Model</em></strong>. Interesting point is <em>GPT</em> only uses <strong>Decoder</strong> layer which has <em>masked self attention layer</em>. Objective of pre-training is given a sequence of words with unlabeled dataset U(u<em>1 … u<em>l) to maximize the likelihood of the probability to predict the word of position \\</em>l</em> given <em>1 … l-1</em> words</p>\n<p><br /> After pre-training, sequence of input data (x*1 to x*m), which would be the first fine-tuning process with <em>dlim token</em>, with label y is given and pass it through pre-trained Transformer model. Then, the output will go through the output linear layer and this would be the second fine-tuning process.</p>"}