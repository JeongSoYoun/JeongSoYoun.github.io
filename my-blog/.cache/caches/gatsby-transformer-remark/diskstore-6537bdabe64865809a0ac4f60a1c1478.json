{"expireTime":9007200906165251000,"key":"transformer-remark-markdown-ast-7061aba261b806990de973832423c461-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7},"indent":[]}},{"type":"html","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAADEElEQVQozx2Si1PSBwCAf3/XdrdVt0e3re20dl5b2qlnE0UReU4FlfmIYfIwYCEIqIFgonCiTmjNlrHZiktNapooGD43a95NduZ94/YPfHfffZ/wxd0tSqaylIUyVPk2ELvXkdnXaDI+Q9f5GH3zA8yNcRzVUYZLQ4Qu+pn5xMsvH7h4fK6f1FkHmTM/cPC+jeMzNoSqyTQ1E5tI7mwh9+f4dnAbjTOHri9Nt36ZntYEFkUMhzjMcGWQ0FdDzF50Mn/BTvJ8Hy8+NJI9+z1/vtfF8TvtCJLRl0iDmf9hCm8OlXOH1v4duqwZDD0pTLoFbOo4LkkYnyhIuGKQ+JV+EiU2nhabWP1Mz6vz3/HXuWby78oQaj0FzYEMMncWR3QPe2iHDvsGHYbn9HQkuamZx6WcxScNExIHmK4eYq7KyUKljcUrvaxd7uZVUQuHn6vIf1SHcNWS4xtLlsEf9xiL7zIzt8vtwDoO+zLxiTVG9A8Za71LRBEhph7nvirIgmqIRc0Q6wY/GZ2T184Qh6VK8pdqEUo69hAZtgjF93EFNvGNppmZShOLrBF2PGH53jpLkymeR1dIx1Jszi6RmfiNrYl5Dh8+JWfychSM8Lqynn8rqhHKNPtc02Rx+beJzmwzXSg+OriC3/6Ee8FFEuNLpOIpkiMJ1qK/84cnTto9zV4wxoF7jDe3xzi+M84/9WJOROWFyvIDahqzqNSrWE2rOIzLdEp/pqdhBqt0nOGWwiqtI4TqbPzUaOORzMILhZEtmZ4DaRtHyibyigZO5Nd4K72MIBLtI6nZQFb9DGXVI7TV9+kSz9JbF8EqCTAg8eCX3mJSZWNO3UeyyczL5l52Wwy80XZyrG3mpK2Bt23lnLYVI9SX7iItW0V5NUlz+Txt5TE6K8IYqgLcFHlx1/cTlFmZUlt4oDWxqDOS7rzBXreBv/Vd5A1aTnqlnJoLQPOnCLKiHPKiFZTFv9J0KY7myzDtJT6ufz1AX8UtPLVWxgqaMY2ZRLeJZZORTfsN9p09HA3oybvbOfHIOfUWgJ6P+Q9k0XsmDcsekgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"openai\" title=\"openai\" src=\"/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png\" srcset=\"/static/7c1f9fb711eecac5f959f784796c2791/5a46d/openai.png 300w,\n/static/7c1f9fb711eecac5f959f784796c2791/0a47e/openai.png 600w,\n/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png 1200w,\n/static/7c1f9fb711eecac5f959f784796c2791/229ad/openai.png 1356w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#introduction","title":null,"children":[],"data":{"hProperties":{"aria-label":"introduction permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Introduction","position":{"start":{"line":4,"column":7,"offset":71},"end":{"line":4,"column":19,"offset":83},"indent":[]}}],"position":{"start":{"line":4,"column":3,"offset":67},"end":{"line":4,"column":19,"offset":83},"indent":[]},"data":{"id":"introduction","htmlAttributes":{"id":"introduction"},"hProperties":{"id":"introduction","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"From Transformer model with ","position":{"start":{"line":6,"column":3,"offset":88},"end":{"line":6,"column":31,"offset":116},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"attention","position":{"start":{"line":6,"column":32,"offset":117},"end":{"line":6,"column":41,"offset":126},"indent":[]}}],"position":{"start":{"line":6,"column":31,"offset":116},"end":{"line":6,"column":42,"offset":127},"indent":[]}},{"type":"text","value":" , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about ","position":{"start":{"line":6,"column":42,"offset":127},"end":{"line":6,"column":319,"offset":404},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":6,"column":322,"offset":407},"end":{"line":6,"column":325,"offset":410},"indent":[]}}],"position":{"start":{"line":6,"column":321,"offset":406},"end":{"line":6,"column":326,"offset":411},"indent":[]}}],"position":{"start":{"line":6,"column":319,"offset":404},"end":{"line":6,"column":328,"offset":413},"indent":[]}},{"type":"text","value":" model.","position":{"start":{"line":6,"column":328,"offset":413},"end":{"line":6,"column":335,"offset":420},"indent":[]}}],"position":{"start":{"line":6,"column":3,"offset":88},"end":{"line":6,"column":335,"offset":420},"indent":[]}}],"position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":6,"column":335,"offset":420},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":8,"column":1,"offset":422},"end":{"line":8,"column":7,"offset":428},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":7,"offset":428},"end":{"line":8,"column":8,"offset":429},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":8,"offset":429},"end":{"line":8,"column":14,"offset":435},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":14,"offset":435},"end":{"line":8,"column":20,"offset":441},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":20,"offset":441},"end":{"line":8,"column":26,"offset":447},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":26,"offset":447},"end":{"line":8,"column":32,"offset":453},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":8,"column":32,"offset":453},"end":{"line":8,"column":38,"offset":459},"indent":[]}},{"type":"text","value":" GPT, ","position":{"start":{"line":8,"column":38,"offset":459},"end":{"line":8,"column":44,"offset":465},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Generative Pre Training","position":{"start":{"line":8,"column":45,"offset":466},"end":{"line":8,"column":68,"offset":489},"indent":[]}}],"position":{"start":{"line":8,"column":44,"offset":465},"end":{"line":8,"column":69,"offset":490},"indent":[]}},{"type":"text","value":", is a language model developed by ","position":{"start":{"line":8,"column":69,"offset":490},"end":{"line":8,"column":104,"offset":525},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Open AI","position":{"start":{"line":8,"column":106,"offset":527},"end":{"line":8,"column":113,"offset":534},"indent":[]}}],"position":{"start":{"line":8,"column":104,"offset":525},"end":{"line":8,"column":115,"offset":536},"indent":[]}},{"type":"text","value":" with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).","position":{"start":{"line":8,"column":115,"offset":536},"end":{"line":8,"column":350,"offset":771},"indent":[]}}],"position":{"start":{"line":8,"column":1,"offset":422},"end":{"line":8,"column":350,"offset":771},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":10,"column":1,"offset":773},"end":{"line":10,"column":7,"offset":779},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#trainig-process","title":null,"children":[],"data":{"hProperties":{"aria-label":"trainig process permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Trainig Process","position":{"start":{"line":12,"column":7,"offset":787},"end":{"line":12,"column":22,"offset":802},"indent":[]}}],"position":{"start":{"line":12,"column":3,"offset":783},"end":{"line":12,"column":22,"offset":802},"indent":[]},"data":{"id":"trainig-process","htmlAttributes":{"id":"trainig-process"},"hProperties":{"id":"trainig-process","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"From pre-training to fine-tuning","position":{"start":{"line":14,"column":3,"offset":807},"end":{"line":14,"column":35,"offset":839},"indent":[]}}],"position":{"start":{"line":14,"column":3,"offset":807},"end":{"line":14,"column":35,"offset":839},"indent":[]}}],"position":{"start":{"line":12,"column":1,"offset":781},"end":{"line":14,"column":35,"offset":839},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":16,"column":1,"offset":841},"end":{"line":16,"column":7,"offset":847},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":7,"offset":847},"end":{"line":16,"column":8,"offset":848},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":8,"offset":848},"end":{"line":16,"column":14,"offset":854},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":14,"offset":854},"end":{"line":16,"column":20,"offset":860},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":20,"offset":860},"end":{"line":16,"column":26,"offset":866},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":26,"offset":866},"end":{"line":16,"column":32,"offset":872},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":16,"column":32,"offset":872},"end":{"line":16,"column":38,"offset":878},"indent":[]}},{"type":"text","value":" First, pre-training(unsupervised learning with set of large-corpus of text data) is done to learn the parameters of language model. Since, the goal of unsupervised learning is to initialize the training, the objectives of learning remain same with supervised learning. After pre-training, supervised learning with same parameters that have pre-trained is processed with fine-tuning of some change of input tokens and the weight of the output layer.","position":{"start":{"line":16,"column":38,"offset":878},"end":{"line":16,"column":487,"offset":1327},"indent":[]}}],"position":{"start":{"line":16,"column":1,"offset":841},"end":{"line":16,"column":487,"offset":1327},"indent":[]}},{"type":"html","value":"<br />","position":{"start":{"line":18,"column":1,"offset":1329},"end":{"line":18,"column":7,"offset":1335},"indent":[]}},{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#architecture","title":null,"children":[],"data":{"hProperties":{"aria-label":"architecture permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Architecture","position":{"start":{"line":20,"column":7,"offset":1343},"end":{"line":20,"column":19,"offset":1355},"indent":[]}}],"position":{"start":{"line":20,"column":3,"offset":1339},"end":{"line":20,"column":19,"offset":1355},"indent":[]},"data":{"id":"architecture","htmlAttributes":{"id":"architecture"},"hProperties":{"id":"architecture","style":"position:relative;"}}}],"position":{"start":{"line":20,"column":1,"offset":1337},"end":{"line":20,"column":19,"offset":1355},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":22,"column":1,"offset":1357},"end":{"line":22,"column":7,"offset":1363},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":7,"offset":1363},"end":{"line":22,"column":8,"offset":1364},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":8,"offset":1364},"end":{"line":22,"column":14,"offset":1370},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":14,"offset":1370},"end":{"line":22,"column":20,"offset":1376},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":20,"offset":1376},"end":{"line":22,"column":26,"offset":1382},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":26,"offset":1382},"end":{"line":22,"column":32,"offset":1388},"indent":[]}},{"type":"text","value":" ","position":{"start":{"line":22,"column":32,"offset":1388},"end":{"line":22,"column":38,"offset":1394},"indent":[]}},{"type":"text","value":" From pre-training to fine-tuning, architecture of GPT is ","position":{"start":{"line":22,"column":38,"offset":1394},"end":{"line":22,"column":96,"offset":1452},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"Transformer Model","position":{"start":{"line":22,"column":99,"offset":1455},"end":{"line":22,"column":116,"offset":1472},"indent":[]}}],"position":{"start":{"line":22,"column":98,"offset":1454},"end":{"line":22,"column":117,"offset":1473},"indent":[]}}],"position":{"start":{"line":22,"column":96,"offset":1452},"end":{"line":22,"column":119,"offset":1475},"indent":[]}},{"type":"text","value":". Interesting point is ","position":{"start":{"line":22,"column":119,"offset":1475},"end":{"line":22,"column":142,"offset":1498},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":22,"column":143,"offset":1499},"end":{"line":22,"column":146,"offset":1502},"indent":[]}}],"position":{"start":{"line":22,"column":142,"offset":1498},"end":{"line":22,"column":147,"offset":1503},"indent":[]}},{"type":"text","value":" only uses ","position":{"start":{"line":22,"column":147,"offset":1503},"end":{"line":22,"column":158,"offset":1514},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Decoder","position":{"start":{"line":22,"column":160,"offset":1516},"end":{"line":22,"column":167,"offset":1523},"indent":[]}}],"position":{"start":{"line":22,"column":158,"offset":1514},"end":{"line":22,"column":169,"offset":1525},"indent":[]}},{"type":"text","value":" layer which has ","position":{"start":{"line":22,"column":169,"offset":1525},"end":{"line":22,"column":186,"offset":1542},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"masked self attention layer","position":{"start":{"line":22,"column":187,"offset":1543},"end":{"line":22,"column":214,"offset":1570},"indent":[]}}],"position":{"start":{"line":22,"column":186,"offset":1542},"end":{"line":22,"column":215,"offset":1571},"indent":[]}},{"type":"text","value":". Objective of pre-training is given a sequence of words with unlabeled dataset U(u","position":{"start":{"line":22,"column":215,"offset":1571},"end":{"line":22,"column":298,"offset":1654},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"1 … u","position":{"start":{"line":22,"column":299,"offset":1655},"end":{"line":22,"column":306,"offset":1662},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"l) to maximize the likelihood of the probability to predict the word of position \\","position":{"start":{"line":22,"column":307,"offset":1663},"end":{"line":22,"column":389,"offset":1745},"indent":[]}}],"position":{"start":{"line":22,"column":306,"offset":1662},"end":{"line":22,"column":390,"offset":1746},"indent":[]}},{"type":"text","value":"l","position":{"start":{"line":22,"column":390,"offset":1746},"end":{"line":22,"column":391,"offset":1747},"indent":[]}}],"position":{"start":{"line":22,"column":298,"offset":1654},"end":{"line":22,"column":392,"offset":1748},"indent":[]}},{"type":"text","value":" given ","position":{"start":{"line":22,"column":392,"offset":1748},"end":{"line":22,"column":399,"offset":1755},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"1 … l-1","position":{"start":{"line":22,"column":400,"offset":1756},"end":{"line":22,"column":409,"offset":1765},"indent":[]}}],"position":{"start":{"line":22,"column":399,"offset":1755},"end":{"line":22,"column":410,"offset":1766},"indent":[]}},{"type":"text","value":" words","position":{"start":{"line":22,"column":410,"offset":1766},"end":{"line":22,"column":416,"offset":1772},"indent":[]}}],"position":{"start":{"line":22,"column":1,"offset":1357},"end":{"line":22,"column":416,"offset":1772},"indent":[]}},{"type":"paragraph","children":[{"type":"html","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1774},"end":{"line":24,"column":7,"offset":1780},"indent":[]}},{"type":"text","value":" After pre-training, sequence of input data (x","position":{"start":{"line":24,"column":7,"offset":1780},"end":{"line":24,"column":53,"offset":1826},"indent":[]}},{"type":"text","value":"*","position":{"start":{"line":24,"column":53,"offset":1826},"end":{"line":24,"column":55,"offset":1828},"indent":[]}},{"type":"text","value":"1 to x","position":{"start":{"line":24,"column":55,"offset":1828},"end":{"line":24,"column":61,"offset":1834},"indent":[]}},{"type":"text","value":"*","position":{"start":{"line":24,"column":61,"offset":1834},"end":{"line":24,"column":63,"offset":1836},"indent":[]}},{"type":"text","value":"m), which would be the first fine-tuning process with ","position":{"start":{"line":24,"column":63,"offset":1836},"end":{"line":24,"column":117,"offset":1890},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"dlim token","position":{"start":{"line":24,"column":118,"offset":1891},"end":{"line":24,"column":128,"offset":1901},"indent":[]}}],"position":{"start":{"line":24,"column":117,"offset":1890},"end":{"line":24,"column":129,"offset":1902},"indent":[]}},{"type":"text","value":", with label y is given and pass it through pre-trained Transformer model. Then, the output will go through the output linear layer and this would be the second fine-tuning process.","position":{"start":{"line":24,"column":129,"offset":1902},"end":{"line":24,"column":310,"offset":2083},"indent":[]}}],"position":{"start":{"line":24,"column":1,"offset":1774},"end":{"line":24,"column":310,"offset":2083},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":25,"column":1,"offset":2084}}}}