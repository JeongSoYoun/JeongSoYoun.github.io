{"expireTime":9007200906165288000,"key":"transformer-remark-markdown-html-ast-1ec6471fd661cf01930bb258a7d3f793-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":7,"offset":7}}},{"type":"raw","value":"<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAADEElEQVQozx2Si1PSBwCAf3/XdrdVt0e3re20dl5b2qlnE0UReU4FlfmIYfIwYCEIqIFgonCiTmjNlrHZiktNapooGD43a95NduZ94/YPfHfffZ/wxd0tSqaylIUyVPk2ELvXkdnXaDI+Q9f5GH3zA8yNcRzVUYZLQ4Qu+pn5xMsvH7h4fK6f1FkHmTM/cPC+jeMzNoSqyTQ1E5tI7mwh9+f4dnAbjTOHri9Nt36ZntYEFkUMhzjMcGWQ0FdDzF50Mn/BTvJ8Hy8+NJI9+z1/vtfF8TvtCJLRl0iDmf9hCm8OlXOH1v4duqwZDD0pTLoFbOo4LkkYnyhIuGKQ+JV+EiU2nhabWP1Mz6vz3/HXuWby78oQaj0FzYEMMncWR3QPe2iHDvsGHYbn9HQkuamZx6WcxScNExIHmK4eYq7KyUKljcUrvaxd7uZVUQuHn6vIf1SHcNWS4xtLlsEf9xiL7zIzt8vtwDoO+zLxiTVG9A8Za71LRBEhph7nvirIgmqIRc0Q6wY/GZ2T184Qh6VK8pdqEUo69hAZtgjF93EFNvGNppmZShOLrBF2PGH53jpLkymeR1dIx1Jszi6RmfiNrYl5Dh8+JWfychSM8Lqynn8rqhHKNPtc02Rx+beJzmwzXSg+OriC3/6Ee8FFEuNLpOIpkiMJ1qK/84cnTto9zV4wxoF7jDe3xzi+M84/9WJOROWFyvIDahqzqNSrWE2rOIzLdEp/pqdhBqt0nOGWwiqtI4TqbPzUaOORzMILhZEtmZ4DaRtHyibyigZO5Nd4K72MIBLtI6nZQFb9DGXVI7TV9+kSz9JbF8EqCTAg8eCX3mJSZWNO3UeyyczL5l52Wwy80XZyrG3mpK2Bt23lnLYVI9SX7iItW0V5NUlz+Txt5TE6K8IYqgLcFHlx1/cTlFmZUlt4oDWxqDOS7rzBXreBv/Vd5A1aTnqlnJoLQPOnCLKiHPKiFZTFv9J0KY7myzDtJT6ufz1AX8UtPLVWxgqaMY2ZRLeJZZORTfsN9p09HA3oybvbOfHIOfUWgJ6P+Q9k0XsmDcsekgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"openai\" title=\"openai\" src=\"/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png\" srcset=\"/static/7c1f9fb711eecac5f959f784796c2791/5a46d/openai.png 300w,\n/static/7c1f9fb711eecac5f959f784796c2791/0a47e/openai.png 600w,\n/static/7c1f9fb711eecac5f959f784796c2791/c1b63/openai.png 1200w,\n/static/7c1f9fb711eecac5f959f784796c2791/229ad/openai.png 1356w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":63,"offset":63}}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":63,"offset":63}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":4,"column":1,"offset":65},"end":{"line":4,"column":7,"offset":71}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"introduction","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#introduction","aria-label":"introduction permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Introduction","position":{"start":{"line":6,"column":7,"offset":79},"end":{"line":6,"column":19,"offset":91}}}],"position":{"start":{"line":6,"column":3,"offset":75},"end":{"line":6,"column":19,"offset":91}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"From Transformer model with ","position":{"start":{"line":8,"column":3,"offset":96},"end":{"line":8,"column":31,"offset":124}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"attention","position":{"start":{"line":8,"column":32,"offset":125},"end":{"line":8,"column":41,"offset":134}}}],"position":{"start":{"line":8,"column":31,"offset":124},"end":{"line":8,"column":42,"offset":135}}},{"type":"text","value":" , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about ","position":{"start":{"line":8,"column":42,"offset":135},"end":{"line":8,"column":319,"offset":412}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"GPT","position":{"start":{"line":8,"column":322,"offset":415},"end":{"line":8,"column":325,"offset":418}}}],"position":{"start":{"line":8,"column":321,"offset":414},"end":{"line":8,"column":326,"offset":419}}}],"position":{"start":{"line":8,"column":319,"offset":412},"end":{"line":8,"column":328,"offset":421}}},{"type":"text","value":" model.","position":{"start":{"line":8,"column":328,"offset":421},"end":{"line":8,"column":335,"offset":428}}}],"position":{"start":{"line":8,"column":3,"offset":96},"end":{"line":8,"column":335,"offset":428}}},{"type":"text","value":"\n"}],"position":{"start":{"line":6,"column":1,"offset":73},"end":{"line":8,"column":335,"offset":428}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":1,"offset":430},"end":{"line":10,"column":7,"offset":436}}},{"type":"raw","value":"<br />","position":{"start":{"line":10,"column":7,"offset":436},"end":{"line":10,"column":13,"offset":442}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":13,"offset":442},"end":{"line":10,"column":14,"offset":443}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":14,"offset":443},"end":{"line":10,"column":20,"offset":449}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":20,"offset":449},"end":{"line":10,"column":26,"offset":455}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":26,"offset":455},"end":{"line":10,"column":32,"offset":461}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":32,"offset":461},"end":{"line":10,"column":38,"offset":467}}},{"type":"text","value":" ","position":{"start":{"line":10,"column":38,"offset":467},"end":{"line":10,"column":44,"offset":473}}},{"type":"text","value":" GPT, ","position":{"start":{"line":10,"column":44,"offset":473},"end":{"line":10,"column":50,"offset":479}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Generative Pre Training","position":{"start":{"line":10,"column":51,"offset":480},"end":{"line":10,"column":74,"offset":503}}}],"position":{"start":{"line":10,"column":50,"offset":479},"end":{"line":10,"column":75,"offset":504}}},{"type":"text","value":", is a language model developed by ","position":{"start":{"line":10,"column":75,"offset":504},"end":{"line":10,"column":110,"offset":539}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Open AI","position":{"start":{"line":10,"column":112,"offset":541},"end":{"line":10,"column":119,"offset":548}}}],"position":{"start":{"line":10,"column":110,"offset":539},"end":{"line":10,"column":121,"offset":550}}},{"type":"text","value":" with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).","position":{"start":{"line":10,"column":121,"offset":550},"end":{"line":10,"column":356,"offset":785}}}],"position":{"start":{"line":10,"column":1,"offset":430},"end":{"line":10,"column":356,"offset":785}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":12,"column":1,"offset":787},"end":{"line":12,"column":7,"offset":793}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"trainig-process","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#trainig-process","aria-label":"trainig process permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Trainig Process","position":{"start":{"line":14,"column":7,"offset":801},"end":{"line":14,"column":22,"offset":816}}}],"position":{"start":{"line":14,"column":3,"offset":797},"end":{"line":14,"column":22,"offset":816}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"From pre-training to fine-tuning","position":{"start":{"line":16,"column":3,"offset":821},"end":{"line":16,"column":35,"offset":853}}}],"position":{"start":{"line":16,"column":3,"offset":821},"end":{"line":16,"column":35,"offset":853}}},{"type":"text","value":"\n"}],"position":{"start":{"line":14,"column":1,"offset":795},"end":{"line":16,"column":35,"offset":853}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":18,"column":1,"offset":855},"end":{"line":18,"column":7,"offset":861}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":7,"offset":861},"end":{"line":18,"column":8,"offset":862}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":8,"offset":862},"end":{"line":18,"column":14,"offset":868}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":14,"offset":868},"end":{"line":18,"column":20,"offset":874}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":20,"offset":874},"end":{"line":18,"column":26,"offset":880}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":26,"offset":880},"end":{"line":18,"column":32,"offset":886}}},{"type":"text","value":" ","position":{"start":{"line":18,"column":32,"offset":886},"end":{"line":18,"column":38,"offset":892}}},{"type":"text","value":" First, pre-training(unsupervised learning with set of large-corpus of text data) is done to learn the parameters of language model. Since, the goal of unsupervised learning is to initialize the training, the objectives of learning remain same with supervised learning. After pre-training, supervised learning with same parameters that have pre-trained is processed with fine-tuning of some change of input tokens and the weight of the output layer.","position":{"start":{"line":18,"column":38,"offset":892},"end":{"line":18,"column":487,"offset":1341}}}],"position":{"start":{"line":18,"column":1,"offset":855},"end":{"line":18,"column":487,"offset":1341}}},{"type":"text","value":"\n"},{"type":"raw","value":"<br />","position":{"start":{"line":20,"column":1,"offset":1343},"end":{"line":20,"column":7,"offset":1349}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"architecture","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#architecture","aria-label":"architecture permalink","class":"anchor before"},"children":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]},{"type":"text","value":"Architecture","position":{"start":{"line":22,"column":7,"offset":1357},"end":{"line":22,"column":19,"offset":1369}}}],"position":{"start":{"line":22,"column":3,"offset":1353},"end":{"line":22,"column":19,"offset":1369}}},{"type":"text","value":"\n"}],"position":{"start":{"line":22,"column":1,"offset":1351},"end":{"line":22,"column":19,"offset":1369}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":24,"column":1,"offset":1371},"end":{"line":24,"column":7,"offset":1377}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":7,"offset":1377},"end":{"line":24,"column":8,"offset":1378}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":8,"offset":1378},"end":{"line":24,"column":14,"offset":1384}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":14,"offset":1384},"end":{"line":24,"column":20,"offset":1390}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":20,"offset":1390},"end":{"line":24,"column":26,"offset":1396}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":26,"offset":1396},"end":{"line":24,"column":32,"offset":1402}}},{"type":"text","value":" ","position":{"start":{"line":24,"column":32,"offset":1402},"end":{"line":24,"column":38,"offset":1408}}},{"type":"text","value":" From pre-training to fine-tuning, architecture of GPT is ","position":{"start":{"line":24,"column":38,"offset":1408},"end":{"line":24,"column":96,"offset":1466}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Transformer Model","position":{"start":{"line":24,"column":99,"offset":1469},"end":{"line":24,"column":116,"offset":1486}}}],"position":{"start":{"line":24,"column":98,"offset":1468},"end":{"line":24,"column":117,"offset":1487}}}],"position":{"start":{"line":24,"column":96,"offset":1466},"end":{"line":24,"column":119,"offset":1489}}},{"type":"text","value":". Interesting point is ","position":{"start":{"line":24,"column":119,"offset":1489},"end":{"line":24,"column":142,"offset":1512}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"GPT","position":{"start":{"line":24,"column":143,"offset":1513},"end":{"line":24,"column":146,"offset":1516}}}],"position":{"start":{"line":24,"column":142,"offset":1512},"end":{"line":24,"column":147,"offset":1517}}},{"type":"text","value":" only uses ","position":{"start":{"line":24,"column":147,"offset":1517},"end":{"line":24,"column":158,"offset":1528}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Decoder","position":{"start":{"line":24,"column":160,"offset":1530},"end":{"line":24,"column":167,"offset":1537}}}],"position":{"start":{"line":24,"column":158,"offset":1528},"end":{"line":24,"column":169,"offset":1539}}},{"type":"text","value":" layer which has ","position":{"start":{"line":24,"column":169,"offset":1539},"end":{"line":24,"column":186,"offset":1556}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"masked self attention layer","position":{"start":{"line":24,"column":187,"offset":1557},"end":{"line":24,"column":214,"offset":1584}}}],"position":{"start":{"line":24,"column":186,"offset":1556},"end":{"line":24,"column":215,"offset":1585}}},{"type":"text","value":". Objective of pre-training is given a sequence of words with unlabeled dataset U(u","position":{"start":{"line":24,"column":215,"offset":1585},"end":{"line":24,"column":298,"offset":1668}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"1 … u","position":{"start":{"line":24,"column":299,"offset":1669},"end":{"line":24,"column":306,"offset":1676}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"l) to maximize the likelihood of the probability to predict the word of position \\","position":{"start":{"line":24,"column":307,"offset":1677},"end":{"line":24,"column":389,"offset":1759}}}],"position":{"start":{"line":24,"column":306,"offset":1676},"end":{"line":24,"column":390,"offset":1760}}},{"type":"text","value":"l","position":{"start":{"line":24,"column":390,"offset":1760},"end":{"line":24,"column":391,"offset":1761}}}],"position":{"start":{"line":24,"column":298,"offset":1668},"end":{"line":24,"column":392,"offset":1762}}},{"type":"text","value":" given ","position":{"start":{"line":24,"column":392,"offset":1762},"end":{"line":24,"column":399,"offset":1769}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"1 … l-1","position":{"start":{"line":24,"column":400,"offset":1770},"end":{"line":24,"column":409,"offset":1779}}}],"position":{"start":{"line":24,"column":399,"offset":1769},"end":{"line":24,"column":410,"offset":1780}}},{"type":"text","value":" words","position":{"start":{"line":24,"column":410,"offset":1780},"end":{"line":24,"column":416,"offset":1786}}}],"position":{"start":{"line":24,"column":1,"offset":1371},"end":{"line":24,"column":416,"offset":1786}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"raw","value":"<br />","position":{"start":{"line":26,"column":1,"offset":1788},"end":{"line":26,"column":7,"offset":1794}}},{"type":"text","value":" After pre-training, sequence of input data (x","position":{"start":{"line":26,"column":7,"offset":1794},"end":{"line":26,"column":53,"offset":1840}}},{"type":"text","value":"*","position":{"start":{"line":26,"column":53,"offset":1840},"end":{"line":26,"column":55,"offset":1842}}},{"type":"text","value":"1 to x","position":{"start":{"line":26,"column":55,"offset":1842},"end":{"line":26,"column":61,"offset":1848}}},{"type":"text","value":"*","position":{"start":{"line":26,"column":61,"offset":1848},"end":{"line":26,"column":63,"offset":1850}}},{"type":"text","value":"m), which would be the first fine-tuning process with ","position":{"start":{"line":26,"column":63,"offset":1850},"end":{"line":26,"column":117,"offset":1904}}},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"dlim token","position":{"start":{"line":26,"column":118,"offset":1905},"end":{"line":26,"column":128,"offset":1915}}}],"position":{"start":{"line":26,"column":117,"offset":1904},"end":{"line":26,"column":129,"offset":1916}}},{"type":"text","value":", with label y is given and pass it through pre-trained Transformer model. Then, the output will go through the output linear layer and this would be the second fine-tuning process.","position":{"start":{"line":26,"column":129,"offset":1916},"end":{"line":26,"column":310,"offset":2097}}}],"position":{"start":{"line":26,"column":1,"offset":1788},"end":{"line":26,"column":310,"offset":2097}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":27,"column":1,"offset":2098}}}}