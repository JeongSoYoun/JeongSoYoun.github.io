{"expireTime":9007200906039833000,"key":"transformer-remark-markdown-ast-2074c4b5bf2ca19e73ca6fe57fd6ca7e-gatsby-remark-katexgatsby-remark-imagesgatsby-remark-images-medium-zoomgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-autolink-headersgatsby-remark-emoji-","val":{"type":"root","children":[{"type":"blockquote","children":[{"type":"heading","depth":3,"children":[{"type":"link","url":"#introduction","title":null,"children":[],"data":{"hProperties":{"aria-label":"introduction permalink","class":"anchor before"},"hChildren":[{"type":"raw","value":"<svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg>"}]}},{"type":"text","value":"Introduction","position":{"start":{"line":2,"column":7,"offset":7},"end":{"line":2,"column":19,"offset":19},"indent":[]}}],"position":{"start":{"line":2,"column":3,"offset":3},"end":{"line":2,"column":19,"offset":19},"indent":[]},"data":{"id":"introduction","htmlAttributes":{"id":"introduction"},"hProperties":{"id":"introduction","style":"position:relative;"}}},{"type":"paragraph","children":[{"type":"text","value":"From Transformer model with ","position":{"start":{"line":4,"column":3,"offset":24},"end":{"line":4,"column":31,"offset":52},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"attention","position":{"start":{"line":4,"column":32,"offset":53},"end":{"line":4,"column":41,"offset":62},"indent":[]}}],"position":{"start":{"line":4,"column":31,"offset":52},"end":{"line":4,"column":42,"offset":63},"indent":[]}},{"type":"text","value":" , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about ","position":{"start":{"line":4,"column":42,"offset":63},"end":{"line":4,"column":319,"offset":340},"indent":[]}},{"type":"strong","children":[{"type":"emphasis","children":[{"type":"text","value":"GPT","position":{"start":{"line":4,"column":322,"offset":343},"end":{"line":4,"column":325,"offset":346},"indent":[]}}],"position":{"start":{"line":4,"column":321,"offset":342},"end":{"line":4,"column":326,"offset":347},"indent":[]}}],"position":{"start":{"line":4,"column":319,"offset":340},"end":{"line":4,"column":328,"offset":349},"indent":[]}},{"type":"text","value":" model.","position":{"start":{"line":4,"column":328,"offset":349},"end":{"line":4,"column":335,"offset":356},"indent":[]}}],"position":{"start":{"line":4,"column":3,"offset":24},"end":{"line":4,"column":335,"offset":356},"indent":[]}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":4,"column":335,"offset":356},"indent":[1,1]}},{"type":"paragraph","children":[{"type":"text","value":"GPT, ","position":{"start":{"line":6,"column":1,"offset":358},"end":{"line":6,"column":6,"offset":363},"indent":[]}},{"type":"emphasis","children":[{"type":"text","value":"Generative Pre Training","position":{"start":{"line":6,"column":7,"offset":364},"end":{"line":6,"column":30,"offset":387},"indent":[]}}],"position":{"start":{"line":6,"column":6,"offset":363},"end":{"line":6,"column":31,"offset":388},"indent":[]}},{"type":"text","value":", is a language model developed by ","position":{"start":{"line":6,"column":31,"offset":388},"end":{"line":6,"column":66,"offset":423},"indent":[]}},{"type":"strong","children":[{"type":"text","value":"Open AI","position":{"start":{"line":6,"column":68,"offset":425},"end":{"line":6,"column":75,"offset":432},"indent":[]}}],"position":{"start":{"line":6,"column":66,"offset":423},"end":{"line":6,"column":77,"offset":434},"indent":[]}},{"type":"text","value":" with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).","position":{"start":{"line":6,"column":77,"offset":434},"end":{"line":6,"column":312,"offset":669},"indent":[]}}],"position":{"start":{"line":6,"column":1,"offset":358},"end":{"line":6,"column":312,"offset":669},"indent":[]}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":7,"column":1,"offset":670}}}}