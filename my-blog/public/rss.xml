<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Yoon's Dev ]]></title><description><![CDATA[Blog posted about ...]]></description><link>https://jeongsoyoun.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Wed, 27 Apr 2022 16:46:53 GMT</lastBuildDate><item><title><![CDATA[Polkadot Dev Camp]]></title><description><![CDATA[Polkadot Official Dev Camp]]></description><link>https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Polkadot Official Dev Camp&lt;/p&gt;</content:encoded></item><item><title><![CDATA[CosmWasm]]></title><link>https://jeongsoyoun.github.io/Blockchain/cosmwasm/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/cosmwasm/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[GPT Model]]></title><description><![CDATA[GPT Model for NLP]]></description><link>https://jeongsoyoun.github.io/ML/AI/gpt/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/gpt/</guid><pubDate>Mon, 25 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;GPT Model for NLP&lt;/p&gt;</content:encoded></item><item><title><![CDATA[about]]></title><description><![CDATA[Thank You !]]></description><link>https://jeongsoyoun.github.io/resume-en/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/resume-en/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;thank-you-&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#thank-you-&quot; aria-label=&quot;thank you  permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thank You !&lt;/h1&gt;
&lt;div&gt;
    Contact
&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Uniswap Core Review]]></title><link>https://jeongsoyoun.github.io/Blockchain/uniswap/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/uniswap/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Bitcoin White Paper]]></title><description><![CDATA[Bitcoin White Paper by Satoshi Nakamoto]]></description><link>https://jeongsoyoun.github.io/Blockchain/bitcoin_white_paper/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/bitcoin_white_paper/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Bitcoin White Paper by Satoshi Nakamoto&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ðŸ¤– Transformer Model]]></title><description><![CDATA[Introduction Transformer Model: Attention is all you Need Â Â Â Â Â Transformer model was published by Google AI on 2017, and became fundamentalâ€¦]]></description><link>https://jeongsoyoun.github.io/ML/AI/transformer/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/transformer/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;br /&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; &quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 52.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz6WRX0/CMBTF+f5fQh/gnSiDESHCg9FgjAlGJeFPhKVra2xj1rK1dm3tcOjCQAKcNGlzkt/tvedW7Amq5LcxxmbnKDgvkdFmXW1Dv2YJ/pKCz5dqmTexWxuwkdrGIaWgWX+eMG6HT8P5IkgSEceJE2NsNnvjnGutx5MpRFgI8Qcn2kaLDxq2a4/TAPDB/aB73b/0Wn670/KvGp5/dl6tX3ghRM65ub1j7odVCxmcWqto9In6DyBwLsIYAIjxO0QIQtTp9l5eR4SQKGJSSkJomqalwLSw29JWSm0duLiqFb6O1I33E6x7FJMvB1ZY1L54/93zgfoGgyp+WM2mC3AAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;google&quot; title=&quot;google&quot; src=&quot;/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png&quot; srcset=&quot;/static/fbc6dd92a8903039c6d1223bd49971b7/5a46d/google.png 300w,
/static/fbc6dd92a8903039c6d1223bd49971b7/0a47e/google.png 600w,
/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png 1200w&quot; sizes=&quot;(max-width: 1200px) 100vw, 1200px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#introduction&quot; aria-label=&quot;introduction permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;transformer-model-attention-is-all-you-need&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#transformer-model-attention-is-all-you-need&quot; aria-label=&quot;transformer model attention is all you need permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Transformer Model: Attention is all you Need&lt;/em&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;Â Â Â Â Â Transformer model was published by &lt;strong&gt;Google AI&lt;/strong&gt; on 2017, and became fundamental of Natural Language Process Model for the other models such as &lt;em&gt;BERT&lt;/em&gt; or &lt;em&gt;GPT&lt;/em&gt;. What makes Transformer model attractive is, this model never uses &lt;strong&gt;RNN&lt;/strong&gt;, one of the greatest sequence model, architecture. It only uses &lt;strong&gt;Attention&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;Â Â Â Â Â Our language has meaningful relationship in sentence, which we called &lt;em&gt;context&lt;/em&gt;. For human, we can easily guess what &lt;em&gt;it&lt;/em&gt; stands for, but for machine learning points of view, it is hard to train what &lt;em&gt;it&lt;/em&gt; means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where &lt;strong&gt;Attention&lt;/strong&gt; shines, and &lt;strong&gt;Transformer&lt;/strong&gt; model only uses this technique.&lt;/p&gt;
&lt;h2 id=&quot;architecture&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#architecture&quot; aria-label=&quot;architecture permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture&lt;/h2&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;input---encoder---deocder---output&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#input---encoder---deocder---output&quot; aria-label=&quot;input   encoder   deocder   output permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Input - Encoder - Deocder - Output&lt;/em&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;Â Â Â Â Â  Transformer model consists of &lt;em&gt;Encoder&lt;/em&gt; layer and &lt;em&gt;Decoder&lt;/em&gt; layer, which is same architecture with &lt;strong&gt;seq2seq&lt;/strong&gt; model, but the inside of each layer has &lt;strong&gt;attention&lt;/strong&gt; layer not &lt;strong&gt;RNN&lt;/strong&gt;. The most important part of attention in Transforme model is &lt;strong&gt;Self-Attention&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt; Self-Attention directly makes Transformer model see relationships between all words in sentence and figured it out which word to attend importantly. This mechanism is done through &lt;em&gt;Encoder&lt;/em&gt; and &lt;em&gt;Decoder&lt;/em&gt; layers. Interesting point is, the output of Encoder goes to Decoder layer and use it for the attention to get the relationship between the input and output for training. Illustration of this process would look like this..!ðŸ˜†&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;img src=&quot;https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif&quot; width=&quot;600&quot; height=&quot;400&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;from-google-ai-blog&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#from-google-ai-blog&quot; aria-label=&quot;from google ai blog permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From: Google AI Blog&lt;/h5&gt;
&lt;p&gt;&lt;br /&gt; Letâ€™s add more explanation for the mechanism. First, all words in sentence do self-attention and get output which has meaningful relationshop among words. After encoding, target word from Decoder do self-attention, but here comes important feature for Transformer comes in. As we can see from the illustration, Decoder of Transformer sees the part of the target word, not the entire sentence. This technique is called &lt;strong&gt;&lt;em&gt;Masked-Self-Attention&lt;/em&gt;&lt;/strong&gt;. Because of this technique, Transformer model predicts &lt;em&gt;i&lt;/em&gt; depends only on the known output at position less than &lt;em&gt;i&lt;/em&gt;. After completing masked-self-attention, we are doing another attention between the output of masked-self-attention and the output of Encoder.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#conclusion&quot; aria-label=&quot;conclusion permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Â Â Â Â Â Transformer model only uses &lt;strong&gt;Self Attention&lt;/strong&gt; for the language model and proves &lt;strong&gt;&lt;em&gt;Attention is all you Need&lt;/em&gt;&lt;/strong&gt; statement. Now, it became fundamental basis of NLP model. It would be excited to see the various application which is built based on Transformer model.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[SubStake]]></title><link>https://jeongsoyoun.github.io/Portfolio/substake/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/substake/</guid><pubDate>Sun, 14 Jun 2020 16:21:13 GMT</pubDate><content:encoded></content:encoded></item></channel></rss>