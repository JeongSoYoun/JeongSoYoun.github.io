<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Yoon's Dev ]]></title><description><![CDATA[Blog posted about ...]]></description><link>https://jeongsoyoun.github.io</link><generator>GatsbyJS</generator><lastBuildDate>Sun, 01 May 2022 07:44:30 GMT</lastBuildDate><item><title><![CDATA[Polkadot Dev Camp]]></title><description><![CDATA[Polkadot Official Dev Camp]]></description><link>https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/polkadot_dev_camp/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Polkadot Official Dev Camp&lt;/p&gt;</content:encoded></item><item><title><![CDATA[CosmWasm]]></title><link>https://jeongsoyoun.github.io/Blockchain/cosmwasm/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/cosmwasm/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Substrate Frame]]></title><description><![CDATA[Introduction  Substrate is blockchain framework that makes developer build their own blockchain easily. All of the Polkadot/Kusamaâ€¦]]></description><link>https://jeongsoyoun.github.io/Blockchain/substrate_frame/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/substrate_frame/</guid><pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;br /&gt;&lt;img src=&quot;/21b64a43b31dc28b05053c6d1c50f067/substrate.gif&quot; width=&quot;600&quot; height=&quot;300&quot;&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt; &lt;strong&gt;Substrate&lt;/strong&gt; is blockchain framework that makes developer build their own blockchain easily. All of the &lt;strong&gt;&lt;em&gt;Polkadot/Kusama parachains&lt;/em&gt;&lt;/strong&gt; are built based on Substrate, which make networks multi-chain platform.&lt;/p&gt;
&lt;p&gt;One of the reason that Substrate makes build blockchain flexible is &lt;strong&gt;FRAME&lt;/strong&gt;, an acronym for &lt;strong&gt;&lt;em&gt;Framework for Runtime Aggregation of Modularized Entities&lt;/em&gt;&lt;/strong&gt;. Here, we can see there is a word &lt;em&gt;Aggregation&lt;/em&gt; meaning there are several modules for Runtime, which called &lt;em&gt;Pallets&lt;/em&gt; that are hosting domain-specific logic.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;Here is overall architecture of Substrate Runtime&lt;br /&gt;
&lt;br /&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; &quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 51.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABAklEQVQoz5VS7YqDMBDs+z+hoBX9IaJEjfkwUavTTCCH9Lj2GljYTDKT2ezeENZ5njGuedpf1w9+uXPlcd1eBZdlgVIqxjiOqOsaVVXFnJhRGnp1UIuFDvtt294L8oL3Hs65QNAQ8xRCQs1zxHi2P3YYYyAniXVd3wuSRCfWWgzDgOJ+R1EUMSfGM4qkSj46TO6SGxIplDDu932HlBJt2352SHIKEsqyjH8ohIhiWuso2Pc9siz7h2AgJVd0IQYRy2V5xE3AvyrZbB7Wh39kJ53FZDXGEMTYXe0XuMcGG5oyyemzw/QyS2uaBnmex6Z0XRc7+3VT/hrs82WYGcdx/BrsJ07aDLbi+vn7AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;runtime architecture&quot; title=&quot;runtime architecture&quot; src=&quot;/static/87d7595109a5517df32d953769f5adbd/c1b63/runtime_architecture.png&quot; srcset=&quot;/static/87d7595109a5517df32d953769f5adbd/5a46d/runtime_architecture.png 300w,
/static/87d7595109a5517df32d953769f5adbd/0a47e/runtime_architecture.png 600w,
/static/87d7595109a5517df32d953769f5adbd/c1b63/runtime_architecture.png 1200w,
/static/87d7595109a5517df32d953769f5adbd/10ab7/runtime_architecture.png 1552w&quot; sizes=&quot;(max-width: 1200px) 100vw, 1200px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h5 id=&quot;source-substrate&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#source-substrate&quot; aria-label=&quot;source substrate permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Source: Substrate&lt;/h5&gt;
&lt;p&gt;&lt;br /&gt; As we can see, Substrate runtime is consist of several modules, which is called &lt;strong&gt;pallet&lt;/strong&gt;. &lt;strong&gt;Pallet&lt;/strong&gt; contains &lt;em&gt;types&lt;/em&gt; for the config for the pallet that are used to logics in pallet, &lt;em&gt;storage items&lt;/em&gt; to store the state of blockchain, &lt;em&gt;functions&lt;/em&gt; for interacting with chain&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;Here are Pallets &lt;br /&gt;
&lt;br /&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; &quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 85.33333333333334%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAABYlAAAWJQFJUiTwAAAC00lEQVQ4y3VU227TQBDNt/LKG0/wA/wFbyDxgsRFfUAISpG4iKZqmhbhNL3kntSO72t7vbv2YWZtp+W20kQ7s+PZM2fPpKe1xmI6hbdeI/JcuIs5lsulNc/zwOf/XjXq+o7XOr3CaPTjCPtegL1tig+JRq40VFkiCEIYw36Fj1caL/s+Xn3z8O6kRFpU0KrJkVKiqipbtFcagwsRYxD4+BomOMoNJMUYWRQntmBWGhzMNN6cRtg7DvH+rCmolEQQhsjznPJMU5ArE3pkcYw8iWlfwSiF0PetJVsPhTJ4vTJ46qR4NozxwjGI6GJVFtjc3CBN01uEvFFUIKGgyDJ7U0KJC5HiMoowzugjQjgXGs5WwLkRuAqUvcRQF1ysJHp2CDsyUyGQF4Xdb7IYx/4GB/RAb4mGkeSHMUCRklEXIL8yNleIBt0OoSokrgcORienuHB+wvXWiPMIc3+O0eoa58EGU+LoyUrh4ecJ7u/PcO+LoA7o4XKBkLpglMx5W7DEdDCC0z/B+OwHvA0VTEPMvRnGy0tMwhWWRYbnNwqPD2d49GmCB/0Mq8ygzAR8P0CSJLcFGXZW5Nj6W5JAAE1cbBdrDL8fYjg8gnN9hjgLbHsyJolEPu0q23JGnLNWBdG1a5l/mNDOKvILkSHxQySkz5QkJbWEpg+0oQckJIrzWt7u2g7h7wNQ/xUaIMIKeTsf/1981mPJTCYTLBYLyFLaA0bZWKPRSU3ko7R7jtd/Gh/UTcVeVmv4OkdkCiRQjdUK9G4QJBX2i9pAtLHuvMuxMdrHFDfcckwOL6OJZCK3WzxOLk2BbLXZaE7YvJ2fCisZuydgkgDYgkSnHXA+7ITO6u/0ZS+kh3Bdd1eAH6GgywrZ0MSdlkRRL2oLciJLoFRqV9AlX7YfMOLz8bn9W+PFF21p1vmSSjf0SKLGcsgoOcDWcdJxeNfvuLvlz7RG/0wUo2nGL4vCHNr5K3FfAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;pallet&quot; title=&quot;pallet&quot; src=&quot;/static/2013ed9c52b5b74cf0bfc0622342a363/c1b63/pallet.png&quot; srcset=&quot;/static/2013ed9c52b5b74cf0bfc0622342a363/5a46d/pallet.png 300w,
/static/2013ed9c52b5b74cf0bfc0622342a363/0a47e/pallet.png 600w,
/static/2013ed9c52b5b74cf0bfc0622342a363/c1b63/pallet.png 1200w,
/static/2013ed9c52b5b74cf0bfc0622342a363/aa440/pallet.png 1500w&quot; sizes=&quot;(max-width: 1200px) 100vw, 1200px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt; If we want our chain to have certain feature, we can just choose the pallet that is related to the feature. For example, if we want Staking, then we choose Staking pallet. Of course, we can build our own Pallet. This is why Substrate is flexible.&lt;/p&gt;
&lt;p&gt;Now, I am going to do Substrate Tutorial called &lt;strong&gt;&lt;em&gt;Kitties Workshop&lt;/em&gt;&lt;/strong&gt;, which is to build full-stack NFT application with our own custom-pallet and some front-end pages&lt;/p&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;p&gt;Kitties Workshop&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Custom Pallet&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Custom Pallet Tutorial&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Front-End&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Front End Tutorial&lt;/p&gt;</content:encoded></item><item><title><![CDATA[GPT Review]]></title><description><![CDATA[Introduction From Transformer model with attention , we have seen that NLP task can be done without RNN architecture, which has broughtâ€¦]]></description><link>https://jeongsoyoun.github.io/ML/AI/gpt/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/gpt/</guid><pubDate>Mon, 25 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;h3 id=&quot;introduction&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#introduction&quot; aria-label=&quot;introduction permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h3&gt;
&lt;p&gt;From Transformer model with &lt;em&gt;attention&lt;/em&gt; , we have seen that NLP task can be done without RNN architecture, which has brought phenomenal development for NLP. This model has become fundamental architecture for NLP models such as BERT and GPT, which are two top tier NLP model nowadays. Today, I am going to talk about &lt;strong&gt;&lt;em&gt;GPT&lt;/em&gt;&lt;/strong&gt; model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt; Â Â Â Â Â  GPT, &lt;em&gt;Generative Pre Training&lt;/em&gt;, is a language model developed by &lt;strong&gt;Open AI&lt;/strong&gt; with combination of generative pre-training and discriminative fine-tuning process. The goal of GPT is making a general language model(pre-training) that can be adapted to lots of tasks with small adjustment of the model(fine-tuning).&lt;/p&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;trainig-process&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#trainig-process&quot; aria-label=&quot;trainig process permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trainig Process&lt;/h3&gt;
&lt;p&gt;From pre-training to fine-tuning&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt; Â Â Â Â Â  First, pre-training(unsupervised learning with set of large-corpus of text data) is done to learn the parameters of language model. Since, the goal of unsupervised learning is to initialize the training, the objectives of learning remain same with supervised learning. After pre-training, supervised learning with same parameters that have pre-trained is processed with fine-tuning of some change of input tokens and the weight of the output layer.&lt;/p&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;architecture&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#architecture&quot; aria-label=&quot;architecture permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt; Â Â Â Â Â  From pre-training to fine-tuning, architecture of GPT is &lt;strong&gt;&lt;em&gt;Transformer Model&lt;/em&gt;&lt;/strong&gt;. Interesting point is &lt;em&gt;GPT&lt;/em&gt; only uses &lt;strong&gt;Decoder&lt;/strong&gt; layer which has &lt;em&gt;masked self attention layer&lt;/em&gt;. Objective of pre-training is given a sequence of words with unlabeled dataset U(u&lt;em&gt;1 â€¦ u&lt;em&gt;l) to maximize the likelihood of the probability to predict the word of position \&lt;/em&gt;l&lt;/em&gt; given &lt;em&gt;1 â€¦ l-1&lt;/em&gt; words.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt; After pre-training, sequence of input data (x*1 to x*m), which would be the first fine-tuning process with &lt;em&gt;dlim token&lt;/em&gt;, with label y is given and pass it through pre-trained Transformer model. Then, the output will go through the output linear layer and this would be the second fine-tuning process.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[about]]></title><description><![CDATA[Thank You !]]></description><link>https://jeongsoyoun.github.io/resume-en/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/resume-en/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;h1 id=&quot;thank-you-&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#thank-you-&quot; aria-label=&quot;thank you  permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Thank You !&lt;/h1&gt;
&lt;div&gt;
    Contact
&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Uniswap Core Review]]></title><link>https://jeongsoyoun.github.io/Blockchain/uniswap/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/uniswap/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded></content:encoded></item><item><title><![CDATA[Bitcoin White Paper]]></title><description><![CDATA[Bitcoin White Paper by Satoshi Nakamoto]]></description><link>https://jeongsoyoun.github.io/Blockchain/bitcoin_white_paper/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Blockchain/bitcoin_white_paper/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Bitcoin White Paper by Satoshi Nakamoto&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ðŸ¤– Transformer Model]]></title><description><![CDATA[Introduction Transformer Model: Attention is all you Need Â Â Â Â Â Transformer model was published by Google AI on 2017, and became fundamentalâ€¦]]></description><link>https://jeongsoyoun.github.io/ML/AI/transformer/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/ML/AI/transformer/</guid><pubDate>Mon, 18 Apr 2022 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;br /&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px; &quot;&gt;
      &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 52.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz6WRX0/CMBTF+f5fQh/gnSiDESHCg9FgjAlGJeFPhKVra2xj1rK1dm3tcOjCQAKcNGlzkt/tvedW7Amq5LcxxmbnKDgvkdFmXW1Dv2YJ/pKCz5dqmTexWxuwkdrGIaWgWX+eMG6HT8P5IkgSEceJE2NsNnvjnGutx5MpRFgI8Qcn2kaLDxq2a4/TAPDB/aB73b/0Wn670/KvGp5/dl6tX3ghRM65ub1j7odVCxmcWqto9In6DyBwLsIYAIjxO0QIQtTp9l5eR4SQKGJSSkJomqalwLSw29JWSm0duLiqFb6O1I33E6x7FJMvB1ZY1L54/93zgfoGgyp+WM2mC3AAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;google&quot; title=&quot;google&quot; src=&quot;/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png&quot; srcset=&quot;/static/fbc6dd92a8903039c6d1223bd49971b7/5a46d/google.png 300w,
/static/fbc6dd92a8903039c6d1223bd49971b7/0a47e/google.png 600w,
/static/fbc6dd92a8903039c6d1223bd49971b7/c1b63/google.png 1200w&quot; sizes=&quot;(max-width: 1200px) 100vw, 1200px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot;&gt;
    &lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#introduction&quot; aria-label=&quot;introduction permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;transformer-model-attention-is-all-you-need&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#transformer-model-attention-is-all-you-need&quot; aria-label=&quot;transformer model attention is all you need permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Transformer Model: Attention is all you Need&lt;/em&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;Â Â Â Â Â Transformer model was published by &lt;strong&gt;Google AI&lt;/strong&gt; on 2017, and became fundamental of Natural Language Process Model for the other models such as &lt;em&gt;BERT&lt;/em&gt; or &lt;em&gt;GPT&lt;/em&gt;. What makes Transformer model attractive is, this model never uses &lt;strong&gt;RNN&lt;/strong&gt;, one of the greatest sequence model, architecture. It only uses &lt;strong&gt;Attention&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;Â Â Â Â Â Our language has meaningful relationship in sentence, which we called &lt;em&gt;context&lt;/em&gt;. For human, we can easily guess what &lt;em&gt;it&lt;/em&gt; stands for, but for machine learning points of view, it is hard to train what &lt;em&gt;it&lt;/em&gt; means in the sentence. The biggest disadvantage of RNN is, this model only depends only short-term memory, so if our input text data set is large, performance of RNN is decreasing. To cover this disadvantage, this is where &lt;strong&gt;Attention&lt;/strong&gt; shines, and &lt;strong&gt;Transformer&lt;/strong&gt; model only uses this technique.&lt;/p&gt;
&lt;h2 id=&quot;architecture&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#architecture&quot; aria-label=&quot;architecture permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Architecture&lt;/h2&gt;
&lt;br /&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;input---encoder---deocder---output&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#input---encoder---deocder---output&quot; aria-label=&quot;input   encoder   deocder   output permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Input - Encoder - Deocder - Output&lt;/em&gt;&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;Â Â Â Â Â  Transformer model consists of &lt;em&gt;Encoder&lt;/em&gt; layer and &lt;em&gt;Decoder&lt;/em&gt; layer, which is same architecture with &lt;strong&gt;seq2seq&lt;/strong&gt; model, but the inside of each layer has &lt;strong&gt;attention&lt;/strong&gt; layer not &lt;strong&gt;RNN&lt;/strong&gt;. The most important part of attention in Transforme model is &lt;strong&gt;Self-Attention&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt; Self-Attention directly makes Transformer model see relationships between all words in sentence and figured it out which word to attend importantly. This mechanism is done through &lt;em&gt;Encoder&lt;/em&gt; and &lt;em&gt;Decoder&lt;/em&gt; layers. Interesting point is, the output of Encoder goes to Decoder layer and use it for the attention to get the relationship between the input and output for training. Illustration of this process would look like this..!ðŸ˜†&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;img src=&quot;https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s1600/transform20fps.gif&quot; width=&quot;600&quot; height=&quot;400&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;from-google-ai-blog&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#from-google-ai-blog&quot; aria-label=&quot;from google ai blog permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From: Google AI Blog&lt;/h5&gt;
&lt;p&gt;&lt;br /&gt; Letâ€™s add more explanation for the mechanism. First, all words in sentence do self-attention and get output which has meaningful relationshop among words. After encoding, target word from Decoder do self-attention, but here comes important feature for Transformer comes in. As we can see from the illustration, Decoder of Transformer sees the part of the target word, not the entire sentence. This technique is called &lt;strong&gt;&lt;em&gt;Masked-Self-Attention&lt;/em&gt;&lt;/strong&gt;. Because of this technique, Transformer model predicts &lt;em&gt;i&lt;/em&gt; depends only on the known output at position less than &lt;em&gt;i&lt;/em&gt;. After completing masked-self-attention, we are doing another attention between the output of masked-self-attention and the output of Encoder.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#conclusion&quot; aria-label=&quot;conclusion permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Â Â Â Â Â Transformer model only uses &lt;strong&gt;Self Attention&lt;/strong&gt; for the language model and proves &lt;strong&gt;&lt;em&gt;Attention is all you Need&lt;/em&gt;&lt;/strong&gt; statement. Now, it became fundamental basis of NLP model. It would be excited to see the various application which is built based on Transformer model.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[SubStake]]></title><link>https://jeongsoyoun.github.io/Portfolio/substake/</link><guid isPermaLink="false">https://jeongsoyoun.github.io/Portfolio/substake/</guid><pubDate>Sun, 14 Jun 2020 16:21:13 GMT</pubDate><content:encoded></content:encoded></item></channel></rss>